{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bbEuB-tHi2P"
   },
   "outputs": [],
   "source": [
    "!pip install captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "RBolrWtdHxCW",
    "outputId": "d6ff32ce-9fff-4a4a-ec31-395d998135f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EznnU7x4IFeh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('drive/My Drive/4位数字验证码识别')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数字验证码生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MqECNLZ_IW95",
    "outputId": "8314884b-d867-4644-f3f1-b79e1f5f12f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Creating image 6000, 6000\n",
      "生成完毕\n"
     ]
    }
   ],
   "source": [
    "# 生成验证码\n",
    "from captcha.image import ImageCaptcha\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random, sys\n",
    "\n",
    "# 错误代码，防止将生成代码再重新执行一遍\n",
    "print(ss)\n",
    "\n",
    "number = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "def random_captcha_text(char_set=number, captcha_size=4):\n",
    "    # 验证码列表\n",
    "    captcha_text = []\n",
    "    for i in range(captcha_size):\n",
    "        c = random.choice(char_set)\n",
    "        captcha_text.append(c)\n",
    "    return captcha_text\n",
    "\n",
    "def get_captcha_text_and_image():\n",
    "    image = ImageCaptcha()\n",
    "    captcha_text = random_captcha_text()    # 返回的是一个列表，如['3', '4', '5', '6']\n",
    "    captcha_text = ''.join(captcha_text)    # 将列表拼接成一个字符串\n",
    "    # 生成验证码\n",
    "    captcha = image.generate(captcha_text)\n",
    "    image.write(captcha_text, './images/'+captcha_text+'.jpg')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    num = 6000\n",
    "    for i in range(num):\n",
    "        get_captcha_text_and_image()\n",
    "        sys.stdout.write('\\r>>Creating image %d, %d'%(i+1, num))    # sys.stdout.write(str) 输出的另一种方式，这种输出不会自动换行\n",
    "        sys.stdout.flush()    # 保证一秒输出一个结果，要不然上面的结果等程序执行完成后才会输出\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "    print('生成完毕')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首次训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CZ4wCMZoJkx_",
    "outputId": "1467ea88-5bc3-44cb-d86a-420627baef52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0:loss=2.302 acc0=0.103 acc1=0.114 acc2=0.148 acc3=0.094 total_acc=0.000\n",
      "Epoch is 1\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1:loss=2.303 acc0=0.102 acc1=0.095 acc2=0.097 acc3=0.094 total_acc=0.000\n",
      "Epoch is 2\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2:loss=2.303 acc0=0.091 acc1=0.095 acc2=0.097 acc3=0.096 total_acc=0.000\n",
      "Epoch is 3\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 3:loss=2.302 acc0=0.093 acc1=0.075 acc2=0.097 acc3=0.094 total_acc=0.000\n",
      "Epoch is 4\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 4:loss=2.287 acc0=0.173 acc1=0.115 acc2=0.104 acc3=0.112 total_acc=0.000\n",
      "Epoch is 5\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 5:loss=2.206 acc0=0.281 acc1=0.181 acc2=0.184 acc3=0.317 total_acc=0.002\n",
      "Epoch is 6\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 6:loss=2.139 acc0=0.358 acc1=0.233 acc2=0.244 acc3=0.430 total_acc=0.010\n",
      "Epoch is 7\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 7:loss=2.049 acc0=0.391 acc1=0.340 acc2=0.336 acc3=0.574 total_acc=0.027\n",
      "Epoch is 8\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 8:loss=1.999 acc0=0.410 acc1=0.425 acc2=0.381 acc3=0.619 total_acc=0.036\n",
      "Epoch is 9\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 9:loss=1.982 acc0=0.443 acc1=0.454 acc2=0.391 acc3=0.617 total_acc=0.050\n",
      "Epoch is 10\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 10:loss=1.955 acc0=0.452 acc1=0.480 acc2=0.423 acc3=0.659 total_acc=0.056\n",
      "Epoch is 11\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 11:loss=1.910 acc0=0.521 acc1=0.523 acc2=0.471 acc3=0.692 total_acc=0.086\n",
      "Epoch is 12\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 12:loss=1.893 acc0=0.546 acc1=0.528 acc2=0.497 acc3=0.701 total_acc=0.105\n",
      "Epoch is 13\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 13:loss=1.885 acc0=0.532 acc1=0.548 acc2=0.492 acc3=0.726 total_acc=0.104\n",
      "Epoch is 14\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 14:loss=1.856 acc0=0.561 acc1=0.582 acc2=0.546 acc3=0.725 total_acc=0.127\n",
      "Epoch is 15\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 15:loss=1.837 acc0=0.605 acc1=0.573 acc2=0.570 acc3=0.725 total_acc=0.134\n",
      "Epoch is 16\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 16:loss=1.816 acc0=0.609 acc1=0.581 acc2=0.656 acc3=0.733 total_acc=0.154\n",
      "Epoch is 17\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 17:loss=1.793 acc0=0.634 acc1=0.601 acc2=0.683 acc3=0.751 total_acc=0.178\n",
      "Epoch is 18\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18:loss=1.804 acc0=0.629 acc1=0.592 acc2=0.623 acc3=0.746 total_acc=0.174\n",
      "Epoch is 19\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 19:loss=1.785 acc0=0.642 acc1=0.611 acc2=0.701 acc3=0.755 total_acc=0.182\n",
      "Epoch is 20\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 20:loss=1.780 acc0=0.656 acc1=0.623 acc2=0.693 acc3=0.746 total_acc=0.196\n",
      "Epoch is 21\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 21:loss=1.785 acc0=0.654 acc1=0.613 acc2=0.691 acc3=0.748 total_acc=0.185\n",
      "Epoch is 22\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 22:loss=1.772 acc0=0.655 acc1=0.624 acc2=0.709 acc3=0.759 total_acc=0.196\n",
      "Epoch is 23\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 23:loss=1.769 acc0=0.664 acc1=0.625 acc2=0.720 acc3=0.753 total_acc=0.211\n",
      "Epoch is 24\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 24:loss=1.772 acc0=0.652 acc1=0.629 acc2=0.709 acc3=0.756 total_acc=0.207\n",
      "Epoch is 25\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 25:loss=1.766 acc0=0.667 acc1=0.631 acc2=0.718 acc3=0.755 total_acc=0.218\n",
      "Epoch is 26\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 26:loss=1.774 acc0=0.647 acc1=0.622 acc2=0.722 acc3=0.748 total_acc=0.203\n",
      "Epoch is 27\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 27:loss=1.763 acc0=0.669 acc1=0.643 acc2=0.723 acc3=0.750 total_acc=0.217\n",
      "Epoch is 28\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 28:loss=1.754 acc0=0.664 acc1=0.636 acc2=0.748 acc3=0.767 total_acc=0.229\n",
      "Epoch is 29\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 29:loss=1.742 acc0=0.730 acc1=0.635 acc2=0.747 acc3=0.759 total_acc=0.247\n",
      "Epoch is 30\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 30:loss=1.726 acc0=0.775 acc1=0.652 acc2=0.757 acc3=0.758 total_acc=0.274\n",
      "Epoch is 31\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 31:loss=1.725 acc0=0.779 acc1=0.657 acc2=0.756 acc3=0.758 total_acc=0.280\n",
      "Epoch is 32\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 32:loss=1.723 acc0=0.773 acc1=0.652 acc2=0.756 acc3=0.761 total_acc=0.273\n",
      "Epoch is 33\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 33:loss=1.718 acc0=0.778 acc1=0.658 acc2=0.761 acc3=0.768 total_acc=0.285\n",
      "Epoch is 34\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 34:loss=1.710 acc0=0.778 acc1=0.689 acc2=0.764 acc3=0.770 total_acc=0.298\n",
      "Epoch is 35\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 35:loss=1.708 acc0=0.785 acc1=0.707 acc2=0.755 acc3=0.769 total_acc=0.309\n",
      "Epoch is 36\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 36:loss=1.703 acc0=0.784 acc1=0.707 acc2=0.773 acc3=0.770 total_acc=0.312\n",
      "Epoch is 37\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 37:loss=1.702 acc0=0.781 acc1=0.718 acc2=0.769 acc3=0.770 total_acc=0.316\n",
      "Epoch is 38\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 38:loss=1.702 acc0=0.784 acc1=0.714 acc2=0.768 acc3=0.770 total_acc=0.312\n",
      "Epoch is 39\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 39:loss=1.703 acc0=0.782 acc1=0.714 acc2=0.770 acc3=0.767 total_acc=0.315\n",
      "Epoch is 40\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 40:loss=1.689 acc0=0.779 acc1=0.707 acc2=0.772 acc3=0.830 total_acc=0.341\n",
      "Epoch is 41\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 41:loss=1.681 acc0=0.786 acc1=0.727 acc2=0.770 acc3=0.851 total_acc=0.358\n",
      "Epoch is 42\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 42:loss=1.684 acc0=0.779 acc1=0.716 acc2=0.764 acc3=0.847 total_acc=0.342\n",
      "Epoch is 43\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 43:loss=1.680 acc0=0.780 acc1=0.719 acc2=0.775 acc3=0.850 total_acc=0.356\n",
      "Epoch is 44\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 44:loss=1.677 acc0=0.782 acc1=0.725 acc2=0.782 acc3=0.851 total_acc=0.361\n",
      "Epoch is 45\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 45:loss=1.675 acc0=0.781 acc1=0.718 acc2=0.788 acc3=0.852 total_acc=0.358\n",
      "Epoch is 46\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 46:loss=1.673 acc0=0.782 acc1=0.723 acc2=0.790 acc3=0.856 total_acc=0.367\n",
      "Epoch is 47\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 47:loss=1.672 acc0=0.801 acc1=0.712 acc2=0.784 acc3=0.853 total_acc=0.369\n",
      "Epoch is 48\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 48:loss=1.661 acc0=0.854 acc1=0.729 acc2=0.769 acc3=0.854 total_acc=0.385\n",
      "Epoch is 49\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 49:loss=1.655 acc0=0.867 acc1=0.728 acc2=0.784 acc3=0.854 total_acc=0.407\n",
      "Epoch is 50\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 50:loss=1.656 acc0=0.866 acc1=0.728 acc2=0.769 acc3=0.858 total_acc=0.399\n",
      "Epoch is 51\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 51:loss=1.662 acc0=0.860 acc1=0.716 acc2=0.769 acc3=0.846 total_acc=0.383\n",
      "Epoch is 52\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 52:loss=1.659 acc0=0.859 acc1=0.729 acc2=0.780 acc3=0.851 total_acc=0.391\n",
      "Epoch is 53\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 53:loss=1.655 acc0=0.863 acc1=0.729 acc2=0.777 acc3=0.852 total_acc=0.397\n",
      "Epoch is 54\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 54:loss=1.653 acc0=0.857 acc1=0.724 acc2=0.795 acc3=0.858 total_acc=0.405\n",
      "Epoch is 55\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 55:loss=1.652 acc0=0.864 acc1=0.725 acc2=0.792 acc3=0.853 total_acc=0.406\n",
      "Epoch is 56\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 56:loss=1.653 acc0=0.866 acc1=0.728 acc2=0.782 acc3=0.855 total_acc=0.408\n",
      "Epoch is 57\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 57:loss=1.649 acc0=0.863 acc1=0.727 acc2=0.798 acc3=0.857 total_acc=0.414\n",
      "Epoch is 58\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 58:loss=1.648 acc0=0.869 acc1=0.732 acc2=0.790 acc3=0.864 total_acc=0.420\n",
      "Epoch is 59\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 59:loss=1.651 acc0=0.870 acc1=0.728 acc2=0.788 acc3=0.856 total_acc=0.415\n",
      "Epoch is 60\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 60:loss=1.646 acc0=0.868 acc1=0.732 acc2=0.797 acc3=0.859 total_acc=0.416\n",
      "Epoch is 61\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 61:loss=1.645 acc0=0.869 acc1=0.735 acc2=0.799 acc3=0.861 total_acc=0.418\n",
      "Epoch is 62\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 62:loss=1.646 acc0=0.871 acc1=0.733 acc2=0.796 acc3=0.867 total_acc=0.423\n",
      "Epoch is 63\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 63:loss=1.645 acc0=0.870 acc1=0.739 acc2=0.799 acc3=0.861 total_acc=0.424\n",
      "Epoch is 64\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 64:loss=1.648 acc0=0.868 acc1=0.740 acc2=0.789 acc3=0.859 total_acc=0.419\n",
      "Epoch is 65\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 65:loss=1.647 acc0=0.868 acc1=0.732 acc2=0.790 acc3=0.864 total_acc=0.416\n",
      "Epoch is 66\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 66:loss=1.646 acc0=0.872 acc1=0.734 acc2=0.796 acc3=0.861 total_acc=0.421\n",
      "Epoch is 67\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 67:loss=1.646 acc0=0.869 acc1=0.728 acc2=0.796 acc3=0.864 total_acc=0.415\n",
      "Epoch is 68\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 68:loss=1.646 acc0=0.871 acc1=0.732 acc2=0.793 acc3=0.866 total_acc=0.419\n",
      "Epoch is 69\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 69:loss=1.646 acc0=0.866 acc1=0.735 acc2=0.795 acc3=0.865 total_acc=0.416\n",
      "Epoch is 70\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 70:loss=1.646 acc0=0.870 acc1=0.732 acc2=0.796 acc3=0.865 total_acc=0.418\n",
      "Epoch is 71\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 71:loss=1.647 acc0=0.868 acc1=0.732 acc2=0.791 acc3=0.860 total_acc=0.412\n",
      "Epoch is 72\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72:loss=1.646 acc0=0.866 acc1=0.735 acc2=0.796 acc3=0.861 total_acc=0.417\n",
      "Epoch is 73\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 73:loss=1.646 acc0=0.872 acc1=0.733 acc2=0.791 acc3=0.865 total_acc=0.417\n",
      "Epoch is 74\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 74:loss=1.647 acc0=0.864 acc1=0.731 acc2=0.791 acc3=0.863 total_acc=0.411\n",
      "Epoch is 75\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 75:loss=1.646 acc0=0.867 acc1=0.741 acc2=0.791 acc3=0.859 total_acc=0.420\n",
      "Epoch is 76\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 76:loss=1.648 acc0=0.866 acc1=0.731 acc2=0.791 acc3=0.860 total_acc=0.411\n",
      "Epoch is 77\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 77:loss=1.645 acc0=0.868 acc1=0.739 acc2=0.792 acc3=0.866 total_acc=0.421\n",
      "Epoch is 78\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 78:loss=1.637 acc0=0.868 acc1=0.768 acc2=0.793 acc3=0.861 total_acc=0.450\n",
      "Epoch is 79\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 79:loss=1.630 acc0=0.869 acc1=0.794 acc2=0.801 acc3=0.861 total_acc=0.473\n",
      "Epoch is 80\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 80:loss=1.625 acc0=0.864 acc1=0.864 acc2=0.788 acc3=0.856 total_acc=0.523\n",
      "Epoch is 81\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 81:loss=1.623 acc0=0.868 acc1=0.808 acc2=0.795 acc3=0.859 total_acc=0.470\n",
      "Epoch is 82\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 82:loss=1.621 acc0=0.864 acc1=0.812 acc2=0.796 acc3=0.864 total_acc=0.471\n",
      "Epoch is 83\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 83:loss=1.626 acc0=0.868 acc1=0.808 acc2=0.793 acc3=0.857 total_acc=0.469\n",
      "Epoch is 84\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 84:loss=1.624 acc0=0.863 acc1=0.817 acc2=0.799 acc3=0.867 total_acc=0.480\n",
      "Epoch is 85\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 85:loss=1.619 acc0=0.868 acc1=0.886 acc2=0.798 acc3=0.863 total_acc=0.546\n",
      "Epoch is 86\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 86:loss=1.618 acc0=0.871 acc1=0.877 acc2=0.790 acc3=0.857 total_acc=0.539\n",
      "Epoch is 87\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 87:loss=1.618 acc0=0.869 acc1=0.878 acc2=0.795 acc3=0.863 total_acc=0.541\n",
      "Epoch is 88\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 88:loss=1.615 acc0=0.865 acc1=0.875 acc2=0.798 acc3=0.865 total_acc=0.544\n",
      "Epoch is 89\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 89:loss=1.611 acc0=0.865 acc1=0.880 acc2=0.800 acc3=0.863 total_acc=0.544\n",
      "Epoch is 90\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 90:loss=1.611 acc0=0.861 acc1=0.883 acc2=0.795 acc3=0.863 total_acc=0.538\n",
      "Epoch is 91\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 91:loss=1.611 acc0=0.866 acc1=0.879 acc2=0.794 acc3=0.865 total_acc=0.542\n",
      "Epoch is 92\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 92:loss=1.610 acc0=0.864 acc1=0.885 acc2=0.796 acc3=0.867 total_acc=0.548\n",
      "Epoch is 93\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 93:loss=1.610 acc0=0.867 acc1=0.881 acc2=0.796 acc3=0.863 total_acc=0.544\n",
      "Epoch is 94\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 94:loss=1.610 acc0=0.868 acc1=0.881 acc2=0.798 acc3=0.861 total_acc=0.544\n",
      "Epoch is 95\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 95:loss=1.611 acc0=0.866 acc1=0.885 acc2=0.797 acc3=0.861 total_acc=0.547\n",
      "Epoch is 96\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 96:loss=1.611 acc0=0.866 acc1=0.884 acc2=0.797 acc3=0.864 total_acc=0.551\n",
      "Epoch is 97\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 97:loss=1.612 acc0=0.867 acc1=0.883 acc2=0.794 acc3=0.863 total_acc=0.546\n",
      "Epoch is 98\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 98:loss=1.614 acc0=0.867 acc1=0.883 acc2=0.795 acc3=0.863 total_acc=0.548\n",
      "Epoch is 99\n",
      "training...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "testing...\n",
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 99:loss=1.614 acc0=0.867 acc1=0.882 acc2=0.797 acc3=0.863 total_acc=0.550\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "# 错误代码，防止将生成代码再重新执行一遍\n",
    "print(ss)\n",
    "\n",
    "# 定义基本参数\n",
    "dataset_dir = './images/'            # 验证码存放文件夹\n",
    "num_test = 0.2                       # 测试集占20%\n",
    "batch_size = 64\n",
    "epochs = 100                         # 训练100个周期\n",
    "num_classes = 10                     # 分类数为10\n",
    "lr = tf.Variable(0.001, dtype=tf.float32)    # 学习率\n",
    "is_training = tf.placeholder(tf.bool)        # 是否为训练状态，tf.bool: True/False\n",
    "\n",
    "\n",
    "# 获取所有验证码图片路径和标签\n",
    "def get_images_paths_and_labels(dataset_dir):\n",
    "    labels = []\n",
    "    image_path = []\n",
    "    for filenames in os.listdir(dataset_dir):\n",
    "        path = os.path.join(dataset_dir, filenames)\n",
    "        image_path.append(path)\n",
    "        image_label = filenames.split('.')[0]    # 如['1234']\n",
    "        image_labels = []                        # 如[1, 2, 3, 4]\n",
    "        for i in range(len(image_label)):\n",
    "            image_labels.append(int(image_label[i]))\n",
    "        labels.append(image_labels)\n",
    "    return image_path, labels\n",
    "\n",
    "\n",
    "image_path, labels = get_images_paths_and_labels(dataset_dir)\n",
    "# 将得到的列表转换为数组，方便后面的打乱数据\n",
    "image_path = np.array(image_path)\n",
    "labels = np.array(labels)\n",
    "# 打乱数据\n",
    "np.random.seed(2019)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(image_path)))    # np.random.permutation()打乱数组，但不在原数组上进行\n",
    "image_path_shuffled = image_path[shuffle_indices]                      # 按打乱的索引重新取数据，即打乱数据\n",
    "labels_shuffled = labels[shuffle_indices]\n",
    "# 切分训练集和测试集\n",
    "test_sample_index = -1*int(num_test*float(len(image_path)))\n",
    "x_train, x_test = image_path_shuffled[:test_sample_index], image_path_shuffled[test_sample_index:]\n",
    "y_train, y_test = labels_shuffled[:test_sample_index], labels_shuffled[test_sample_index:]\n",
    "\n",
    "\n",
    "# 图像处理函数\n",
    "def parse_function(filenames, images_labels=None):\n",
    "    image = tf.read_file(filenames)                    # 读取文件\n",
    "    image = tf.image.decode_jpeg(image, channels=3)    # 对图像文件解码\n",
    "    image = tf.image.resize_images(image, [224, 224])  # 将图片重新定义大小，因为后面用的是AlexNet网络，图片大小为224*224\n",
    "    # 图片预处理\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "    return image, images_labels\n",
    "\n",
    "\n",
    "features_placeholder = tf.placeholder(image_path_shuffled.dtype, [None])\n",
    "labels_placeholder = tf.placeholder(labels_shuffled.dtype, [None, 4])\n",
    "# 创建dataset对象, 类似于管道机制，需要配合Iterator进行使用\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "dataset = dataset.map(parse_function)    # 处理图片\n",
    "dataset = dataset.repeat(1)              # 训练周期为1，即将所有数据遍历一次\n",
    "dataset = dataset.batch(batch_size)      # 批次大小\n",
    "iterator = dataset.make_initializable_iterator()    # 初始化迭代器\n",
    "data_batch, label_batch = iterator.get_next()\n",
    "\n",
    "\n",
    "def AlexNet(inputs, is_training=True):\n",
    "    '''定义AlexNet神经网络结构，该网络获得2012年的图像识别冠军'''\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=tf.glorot_uniform_initializer(),\n",
    "                        biases_initializer=tf.constant_initializer(0)):    # 会自动初始化所有的权重\n",
    "        # 首先是常见使用的tf.nn.conv2d的函数，其定义如下：\n",
    "        # conv2d(input,filter,strides,padding,use_cudnn_on_gpu=None,data_format=None,name=None)\n",
    "        # 而对于tf.contrib.slim.conv2d，其函数定义如下：\n",
    "        # convolution(inputs,num_outputs,kernel_size,stride=1,padding='SAME',data_format=None,rate=1,\n",
    "        #             activation_fn=nn.relu,normalizer_fn=None,normalizer_params=None,\n",
    "        #             weights_initializer=initializers.xavier_initializer(),weights_regularizer=None,\n",
    "        #             biases_initializer=init_ops.zeros_initializer(),\n",
    "        #             biases_regularizer=None,reuse=None,\n",
    "        #             variables_collections=None,\n",
    "        #             outputs_collections=None,\n",
    "        #             trainable=True,scope=None)\n",
    "        # slim.max_pool2d(inputs,kernel_size,stride=2,padding='VALID',data_format=DATA_FORMAT_NHWC,\n",
    "        #            outputs_collections=None,scope=None)\n",
    "\n",
    "#         AlexNet网络，但是精确度不高\n",
    "#         net = slim.conv2d(inputs, 96, [11, 11], 4)\n",
    "#         net = slim.max_pool2d(net, [3, 3])\n",
    "#         net = slim.conv2d(net, 256, [5, 5])\n",
    "#         net = slim.max_pool2d(net, [3, 3])\n",
    "#         net = slim.conv2d(net, 384, [5, 5])\n",
    "#         net = slim.conv2d(net, 384, [3, 3])\n",
    "#         net = slim.conv2d(net, 256, [3, 3])\n",
    "#         net = slim.max_pool2d(net, [3, 3])\n",
    "\n",
    "        # 自定义的神经网络结构\n",
    "        net = slim.conv2d(inputs, 64, [11, 11], 4)    # 4: stride\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        net = slim.conv2d(net, 192, [5, 5])\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        \n",
    "        # 数据扁平化\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 4096)\n",
    "        net = slim.dropout(net, is_training=is_training)\n",
    "\n",
    "        net0 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net1 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net2 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net3 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "\n",
    "    return net0, net1, net2, net3\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 传入数据得到结果\n",
    "    logit0, logit1, logit2, logit3 = AlexNet(data_batch, is_training)\n",
    "    # 定义loss\n",
    "    ## sparse_softmax_cross_etropy: 标签为整数；\n",
    "    ## softmax_cross_entropy: 标签为one-hot编码\n",
    "    loss0 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 0], logit0)\n",
    "    loss1 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 1], logit1)\n",
    "    loss2 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 2], logit2)\n",
    "    loss3 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 3], logit3)\n",
    "    total_loss = (loss0 + loss1 + loss2 + loss3) / 4.0\n",
    "\n",
    "    # 优化total_loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(total_loss)\n",
    "\n",
    "    correct0 = tf.nn.in_top_k(logit0, label_batch[:, 0], 1)\n",
    "    accuracy0 = tf.reduce_mean(tf.cast(correct0, tf.float32))\n",
    "    correct1 = tf.nn.in_top_k(logit1, label_batch[:, 1], 1)\n",
    "    accuracy1 = tf.reduce_mean(tf.cast(correct1, tf.float32))\n",
    "    correct2 = tf.nn.in_top_k(logit2, label_batch[:, 2], 1)\n",
    "    accuracy2 = tf.reduce_mean(tf.cast(correct2, tf.float32))\n",
    "    correct3 = tf.nn.in_top_k(logit3, label_batch[:, 3], 1)\n",
    "    accuracy3 = tf.reduce_mean(tf.cast(correct3, tf.float32))\n",
    "    total_correct = tf.cast(correct0, tf.float32)*tf.cast(correct1, tf.float32)*tf.cast(correct2, tf.float32)*tf.cast(correct3, tf.float32)\n",
    "    total_accuracy = tf.reduce_mean(tf.cast(total_correct, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    for i in range(epochs):\n",
    "        if i%30==0:\n",
    "            sess.run(tf.assign(lr, lr/3))\n",
    "\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: x_train, labels_placeholder: y_train})\n",
    "        print('Epoch is '+ str(i))\n",
    "        print('training...')\n",
    "        l = 1\n",
    "        while True:\n",
    "            try:\n",
    "                l+=1\n",
    "                print(l, end=' ')\n",
    "                sess.run(optimizer, feed_dict={is_training: True})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # 训练完所有的数据后则跳出循环\n",
    "                break\n",
    "\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: x_test, labels_placeholder: y_test})\n",
    "        print()\n",
    "        print('testing...')\n",
    "        j = 1\n",
    "        while True:\n",
    "            try:\n",
    "                j+=1\n",
    "                print(j, end=' ')\n",
    "                acc0, acc1, acc2, acc3, total_acc, los = sess.run([accuracy0, accuracy1, accuracy2, accuracy3,\n",
    "                                                                   total_accuracy, total_loss],\n",
    "                                                                  feed_dict={is_training: False})\n",
    "                tf.add_to_collection('sum_losses', los)    # tf.add_to_collection()把多个变量放入一个自己y用引号命名的集合里，也就是把多个变量统一放在一个列表中\n",
    "                tf.add_to_collection('accuracy0', acc0)\n",
    "                tf.add_to_collection('accuracy1', acc1)\n",
    "                tf.add_to_collection('accuracy2', acc2)\n",
    "                tf.add_to_collection('accuracy3', acc3)\n",
    "                tf.add_to_collection('total_accuracy', total_acc)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                avg_loss = sess.run(tf.reduce_mean(tf.get_collection('sum_losses')))    # tf.get_collection()从一个结合中取出全部变量，是一个列表\n",
    "                avg_acc0 = sess.run(tf.reduce_mean(tf.get_collection('accuracy0')))\n",
    "                avg_acc1 = sess.run(tf.reduce_mean(tf.get_collection('accuracy1')))\n",
    "                avg_acc2 = sess.run(tf.reduce_mean(tf.get_collection('accuracy2')))\n",
    "                avg_acc3 = sess.run(tf.reduce_mean(tf.get_collection('accuracy3')))\n",
    "                avg_total_acc = sess.run(tf.reduce_mean(tf.get_collection('total_accuracy')))\n",
    "                print('%d:loss=%.3f acc0=%.3f acc1=%.3f acc2=%.3f acc3=%.3f total_acc=%.3f' %(i,avg_loss,avg_acc0,avg_acc1,avg_acc2,avg_acc3,avg_total_acc))\n",
    "                temp = tf.get_collection_ref('sum_losses')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy0')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy1')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy2')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy3')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('total_accuracy')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                break\n",
    "        saver.save(sess, 'models/model.ckpt')\n",
    "#         saver.save(sess, 'models/model.ckpt', global_step=10)    # global_step: 在n次迭代后再保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 总结\n",
    "训练100个周期后的结果并不是很好，而且耗时较长，可以选择一些更好的网络结构进行模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 总结\n",
    "使用课程的原代码训练250个周期后，总的准确率达到0.751，虽然效果有所提升，但是在后期，模型提升较慢，这是为什么？？？    \n",
    "模型迭代次数不够\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最后调试代码得到的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c82CJIR3iPNB",
    "outputId": "58a5d08b-6d5b-4017-95e0-3752e489456c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:loss=2.304 acc0=0.103 acc1=0.093 acc2=0.146 acc3=0.098 total_acc=0.000\n",
      "1:loss=2.303 acc0=0.097 acc1=0.094 acc2=0.146 acc3=0.091 total_acc=0.000\n",
      "2:loss=2.303 acc0=0.097 acc1=0.094 acc2=0.146 acc3=0.091 total_acc=0.000\n",
      "3:loss=2.303 acc0=0.097 acc1=0.094 acc2=0.146 acc3=0.106 total_acc=0.000\n",
      "4:loss=2.304 acc0=0.097 acc1=0.091 acc2=0.146 acc3=0.091 total_acc=0.000\n",
      "5:loss=2.305 acc0=0.101 acc1=0.074 acc2=0.104 acc3=0.106 total_acc=0.000\n",
      "6:loss=2.305 acc0=0.101 acc1=0.094 acc2=0.146 acc3=0.106 total_acc=0.000\n",
      "7:loss=2.304 acc0=0.087 acc1=0.094 acc2=0.145 acc3=0.114 total_acc=0.000\n",
      "8:loss=2.258 acc0=0.158 acc1=0.155 acc2=0.221 acc3=0.195 total_acc=0.001\n",
      "9:loss=2.170 acc0=0.235 acc1=0.254 acc2=0.280 acc3=0.357 total_acc=0.006\n",
      "10:loss=2.093 acc0=0.375 acc1=0.383 acc2=0.297 acc3=0.387 total_acc=0.015\n",
      "11:loss=2.003 acc0=0.475 acc1=0.415 acc2=0.431 acc3=0.492 total_acc=0.040\n",
      "12:loss=1.979 acc0=0.512 acc1=0.419 acc2=0.477 acc3=0.510 total_acc=0.050\n",
      "13:loss=1.978 acc0=0.527 acc1=0.384 acc2=0.496 acc3=0.498 total_acc=0.060\n",
      "14:loss=1.923 acc0=0.583 acc1=0.486 acc2=0.530 acc3=0.541 total_acc=0.079\n",
      "15:loss=1.904 acc0=0.593 acc1=0.475 acc2=0.577 acc3=0.565 total_acc=0.102\n",
      "16:loss=1.866 acc0=0.626 acc1=0.542 acc2=0.659 acc3=0.547 total_acc=0.119\n",
      "17:loss=1.842 acc0=0.628 acc1=0.580 acc2=0.704 acc3=0.564 total_acc=0.157\n",
      "18:loss=1.853 acc0=0.632 acc1=0.499 acc2=0.724 acc3=0.555 total_acc=0.153\n",
      "19:loss=1.822 acc0=0.647 acc1=0.609 acc2=0.735 acc3=0.585 total_acc=0.181\n",
      "20:loss=1.812 acc0=0.645 acc1=0.634 acc2=0.741 acc3=0.574 total_acc=0.194\n",
      "21:loss=1.795 acc0=0.649 acc1=0.652 acc2=0.785 acc3=0.576 total_acc=0.196\n",
      "22:loss=1.791 acc0=0.651 acc1=0.645 acc2=0.781 acc3=0.597 total_acc=0.215\n",
      "23:loss=1.793 acc0=0.669 acc1=0.586 acc2=0.800 acc3=0.585 total_acc=0.223\n",
      "24:loss=1.787 acc0=0.666 acc1=0.663 acc2=0.795 acc3=0.579 total_acc=0.214\n",
      "25:loss=1.778 acc0=0.669 acc1=0.655 acc2=0.802 acc3=0.598 total_acc=0.224\n",
      "26:loss=1.795 acc0=0.670 acc1=0.598 acc2=0.802 acc3=0.589 total_acc=0.229\n",
      "27:loss=1.784 acc0=0.674 acc1=0.597 acc2=0.823 acc3=0.601 total_acc=0.247\n",
      "28:loss=1.770 acc0=0.667 acc1=0.675 acc2=0.815 acc3=0.601 total_acc=0.236\n",
      "29:loss=1.768 acc0=0.676 acc1=0.677 acc2=0.817 acc3=0.601 total_acc=0.245\n",
      "30:loss=1.764 acc0=0.679 acc1=0.670 acc2=0.826 acc3=0.610 total_acc=0.239\n",
      "31:loss=1.773 acc0=0.665 acc1=0.602 acc2=0.815 acc3=0.647 total_acc=0.242\n",
      "32:loss=1.754 acc0=0.675 acc1=0.672 acc2=0.841 acc3=0.663 total_acc=0.262\n",
      "33:loss=1.750 acc0=0.672 acc1=0.680 acc2=0.837 acc3=0.665 total_acc=0.271\n",
      "34:loss=1.737 acc0=0.682 acc1=0.684 acc2=0.859 acc3=0.677 total_acc=0.292\n",
      "35:loss=1.741 acc0=0.680 acc1=0.688 acc2=0.835 acc3=0.678 total_acc=0.277\n",
      "36:loss=1.750 acc0=0.682 acc1=0.637 acc2=0.836 acc3=0.688 total_acc=0.299\n",
      "37:loss=1.724 acc0=0.768 acc1=0.631 acc2=0.850 acc3=0.695 total_acc=0.342\n",
      "38:loss=1.709 acc0=0.790 acc1=0.699 acc2=0.854 acc3=0.677 total_acc=0.347\n",
      "39:loss=1.704 acc0=0.792 acc1=0.696 acc2=0.854 acc3=0.701 total_acc=0.355\n",
      "40:loss=1.717 acc0=0.785 acc1=0.631 acc2=0.837 acc3=0.691 total_acc=0.343\n",
      "41:loss=1.704 acc0=0.783 acc1=0.703 acc2=0.853 acc3=0.691 total_acc=0.342\n",
      "42:loss=1.697 acc0=0.791 acc1=0.703 acc2=0.867 acc3=0.693 total_acc=0.354\n",
      "43:loss=1.709 acc0=0.790 acc1=0.694 acc2=0.828 acc3=0.682 total_acc=0.321\n",
      "44:loss=1.706 acc0=0.783 acc1=0.708 acc2=0.855 acc3=0.692 total_acc=0.344\n",
      "45:loss=1.696 acc0=0.792 acc1=0.702 acc2=0.869 acc3=0.701 total_acc=0.354\n",
      "46:loss=1.694 acc0=0.793 acc1=0.703 acc2=0.876 acc3=0.693 total_acc=0.354\n",
      "47:loss=1.697 acc0=0.794 acc1=0.701 acc2=0.878 acc3=0.693 total_acc=0.354\n",
      "48:loss=1.702 acc0=0.778 acc1=0.696 acc2=0.869 acc3=0.699 total_acc=0.350\n",
      "49:loss=1.663 acc0=0.790 acc1=0.697 acc2=0.864 acc3=0.845 total_acc=0.387\n",
      "50:loss=1.662 acc0=0.794 acc1=0.704 acc2=0.869 acc3=0.853 total_acc=0.400\n",
      "51:loss=1.681 acc0=0.799 acc1=0.647 acc2=0.874 acc3=0.791 total_acc=0.409\n",
      "52:loss=1.666 acc0=0.796 acc1=0.645 acc2=0.870 acc3=0.852 total_acc=0.402\n",
      "53:loss=1.652 acc0=0.794 acc1=0.709 acc2=0.873 acc3=0.860 total_acc=0.408\n",
      "54:loss=1.629 acc0=0.934 acc1=0.652 acc2=0.883 acc3=0.857 total_acc=0.448\n",
      "55:loss=1.608 acc0=0.950 acc1=0.715 acc2=0.895 acc3=0.863 total_acc=0.539\n",
      "56:loss=1.634 acc0=0.948 acc1=0.646 acc2=0.864 acc3=0.849 total_acc=0.441\n",
      "57:loss=1.618 acc0=0.946 acc1=0.697 acc2=0.866 acc3=0.869 total_acc=0.494\n",
      "58:loss=1.616 acc0=0.941 acc1=0.694 acc2=0.871 acc3=0.870 total_acc=0.496\n",
      "59:loss=1.596 acc0=0.942 acc1=0.774 acc2=0.881 acc3=0.867 total_acc=0.574\n",
      "60:loss=1.610 acc0=0.939 acc1=0.706 acc2=0.887 acc3=0.857 total_acc=0.507\n",
      "61:loss=1.595 acc0=0.945 acc1=0.784 acc2=0.881 acc3=0.852 total_acc=0.573\n",
      "62:loss=1.593 acc0=0.940 acc1=0.779 acc2=0.887 acc3=0.860 total_acc=0.571\n",
      "63:loss=1.613 acc0=0.934 acc1=0.712 acc2=0.885 acc3=0.859 total_acc=0.507\n",
      "64:loss=1.592 acc0=0.942 acc1=0.793 acc2=0.878 acc3=0.866 total_acc=0.585\n",
      "65:loss=1.606 acc0=0.938 acc1=0.728 acc2=0.894 acc3=0.860 total_acc=0.519\n",
      "66:loss=1.597 acc0=0.940 acc1=0.777 acc2=0.882 acc3=0.856 total_acc=0.559\n",
      "67:loss=1.594 acc0=0.951 acc1=0.776 acc2=0.891 acc3=0.866 total_acc=0.590\n",
      "68:loss=1.608 acc0=0.938 acc1=0.708 acc2=0.897 acc3=0.863 total_acc=0.516\n",
      "69:loss=1.606 acc0=0.941 acc1=0.724 acc2=0.895 acc3=0.857 total_acc=0.519\n",
      "70:loss=1.608 acc0=0.941 acc1=0.730 acc2=0.882 acc3=0.863 total_acc=0.515\n",
      "71:loss=1.605 acc0=0.933 acc1=0.777 acc2=0.867 acc3=0.844 total_acc=0.545\n",
      "72:loss=1.606 acc0=0.936 acc1=0.775 acc2=0.873 acc3=0.855 total_acc=0.561\n",
      "73:loss=1.598 acc0=0.941 acc1=0.714 acc2=0.896 acc3=0.866 total_acc=0.519\n",
      "74:loss=1.608 acc0=0.942 acc1=0.734 acc2=0.882 acc3=0.857 total_acc=0.519\n",
      "75:loss=1.590 acc0=0.946 acc1=0.798 acc2=0.893 acc3=0.859 total_acc=0.587\n",
      "76:loss=1.589 acc0=0.942 acc1=0.792 acc2=0.896 acc3=0.859 total_acc=0.580\n",
      "77:loss=1.592 acc0=0.935 acc1=0.796 acc2=0.892 acc3=0.860 total_acc=0.587\n",
      "78:loss=1.612 acc0=0.929 acc1=0.720 acc2=0.883 acc3=0.859 total_acc=0.510\n",
      "79:loss=1.590 acc0=0.945 acc1=0.786 acc2=0.889 acc3=0.863 total_acc=0.591\n",
      "80:loss=1.589 acc0=0.941 acc1=0.802 acc2=0.889 acc3=0.861 total_acc=0.586\n",
      "81:loss=1.590 acc0=0.940 acc1=0.799 acc2=0.901 acc3=0.849 total_acc=0.592\n",
      "82:loss=1.603 acc0=0.943 acc1=0.732 acc2=0.895 acc3=0.855 total_acc=0.529\n",
      "83:loss=1.588 acc0=0.945 acc1=0.790 acc2=0.901 acc3=0.857 total_acc=0.599\n",
      "84:loss=1.592 acc0=0.938 acc1=0.790 acc2=0.893 acc3=0.856 total_acc=0.578\n",
      "85:loss=1.602 acc0=0.945 acc1=0.727 acc2=0.896 acc3=0.863 total_acc=0.523\n",
      "86:loss=1.603 acc0=0.941 acc1=0.736 acc2=0.889 acc3=0.860 total_acc=0.522\n",
      "87:loss=1.597 acc0=0.946 acc1=0.780 acc2=0.885 acc3=0.858 total_acc=0.585\n",
      "88:loss=1.595 acc0=0.942 acc1=0.797 acc2=0.870 acc3=0.864 total_acc=0.586\n",
      "89:loss=1.611 acc0=0.940 acc1=0.724 acc2=0.881 acc3=0.854 total_acc=0.521\n",
      "90:loss=1.606 acc0=0.942 acc1=0.730 acc2=0.882 acc3=0.864 total_acc=0.522\n",
      "91:loss=1.598 acc0=0.953 acc1=0.727 acc2=0.893 acc3=0.869 total_acc=0.539\n",
      "92:loss=1.603 acc0=0.941 acc1=0.722 acc2=0.899 acc3=0.864 total_acc=0.524\n",
      "93:loss=1.598 acc0=0.955 acc1=0.734 acc2=0.901 acc3=0.864 total_acc=0.548\n",
      "94:loss=1.601 acc0=0.948 acc1=0.732 acc2=0.895 acc3=0.861 total_acc=0.533\n",
      "95:loss=1.599 acc0=0.949 acc1=0.735 acc2=0.898 acc3=0.865 total_acc=0.531\n",
      "96:loss=1.598 acc0=0.958 acc1=0.725 acc2=0.903 acc3=0.866 total_acc=0.534\n",
      "97:loss=1.593 acc0=0.957 acc1=0.740 acc2=0.905 acc3=0.869 total_acc=0.548\n",
      "98:loss=1.588 acc0=0.955 acc1=0.796 acc2=0.881 acc3=0.861 total_acc=0.580\n",
      "99:loss=1.604 acc0=0.946 acc1=0.734 acc2=0.882 acc3=0.859 total_acc=0.539\n",
      "100:loss=1.602 acc0=0.952 acc1=0.735 acc2=0.885 acc3=0.867 total_acc=0.531\n",
      "101:loss=1.601 acc0=0.948 acc1=0.728 acc2=0.891 acc3=0.870 total_acc=0.538\n",
      "102:loss=1.599 acc0=0.946 acc1=0.741 acc2=0.895 acc3=0.864 total_acc=0.538\n",
      "103:loss=1.609 acc0=0.944 acc1=0.727 acc2=0.884 acc3=0.852 total_acc=0.500\n",
      "104:loss=1.607 acc0=0.952 acc1=0.734 acc2=0.891 acc3=0.835 total_acc=0.503\n",
      "105:loss=1.601 acc0=0.953 acc1=0.734 acc2=0.891 acc3=0.859 total_acc=0.528\n",
      "106:loss=1.592 acc0=0.947 acc1=0.792 acc2=0.892 acc3=0.866 total_acc=0.581\n",
      "107:loss=1.584 acc0=0.951 acc1=0.794 acc2=0.899 acc3=0.858 total_acc=0.594\n",
      "108:loss=1.607 acc0=0.938 acc1=0.716 acc2=0.891 acc3=0.864 total_acc=0.515\n",
      "109:loss=1.603 acc0=0.951 acc1=0.740 acc2=0.892 acc3=0.848 total_acc=0.533\n",
      "110:loss=1.599 acc0=0.952 acc1=0.739 acc2=0.899 acc3=0.855 total_acc=0.533\n",
      "111:loss=1.602 acc0=0.949 acc1=0.742 acc2=0.885 acc3=0.863 total_acc=0.529\n",
      "112:loss=1.599 acc0=0.957 acc1=0.728 acc2=0.904 acc3=0.858 total_acc=0.528\n",
      "113:loss=1.599 acc0=0.959 acc1=0.732 acc2=0.905 acc3=0.851 total_acc=0.530\n",
      "114:loss=1.603 acc0=0.954 acc1=0.729 acc2=0.901 acc3=0.848 total_acc=0.528\n",
      "115:loss=1.602 acc0=0.962 acc1=0.726 acc2=0.882 acc3=0.864 total_acc=0.533\n",
      "116:loss=1.599 acc0=0.957 acc1=0.735 acc2=0.899 acc3=0.858 total_acc=0.542\n",
      "117:loss=1.581 acc0=0.952 acc1=0.810 acc2=0.895 acc3=0.866 total_acc=0.611\n",
      "118:loss=1.613 acc0=0.948 acc1=0.728 acc2=0.833 acc3=0.866 total_acc=0.535\n",
      "119:loss=1.603 acc0=0.948 acc1=0.738 acc2=0.891 acc3=0.856 total_acc=0.528\n",
      "120:loss=1.607 acc0=0.941 acc1=0.716 acc2=0.899 acc3=0.857 total_acc=0.519\n",
      "121:loss=1.601 acc0=0.949 acc1=0.782 acc2=0.886 acc3=0.846 total_acc=0.573\n",
      "122:loss=1.605 acc0=0.952 acc1=0.727 acc2=0.889 acc3=0.853 total_acc=0.519\n",
      "123:loss=1.601 acc0=0.949 acc1=0.730 acc2=0.898 acc3=0.857 total_acc=0.535\n",
      "124:loss=1.601 acc0=0.955 acc1=0.731 acc2=0.894 acc3=0.853 total_acc=0.527\n",
      "125:loss=1.596 acc0=0.959 acc1=0.736 acc2=0.900 acc3=0.860 total_acc=0.555\n",
      "126:loss=1.570 acc0=0.953 acc1=0.867 acc2=0.892 acc3=0.848 total_acc=0.635\n",
      "127:loss=1.560 acc0=0.958 acc1=0.878 acc2=0.906 acc3=0.860 total_acc=0.666\n",
      "128:loss=1.592 acc0=0.929 acc1=0.791 acc2=0.898 acc3=0.855 total_acc=0.566\n",
      "129:loss=1.571 acc0=0.954 acc1=0.868 acc2=0.883 acc3=0.853 total_acc=0.635\n",
      "130:loss=1.574 acc0=0.943 acc1=0.868 acc2=0.897 acc3=0.841 total_acc=0.640\n",
      "131:loss=1.622 acc0=0.918 acc1=0.785 acc2=0.869 acc3=0.779 total_acc=0.531\n",
      "132:loss=1.563 acc0=0.953 acc1=0.887 acc2=0.896 acc3=0.855 total_acc=0.663\n",
      "133:loss=1.571 acc0=0.943 acc1=0.870 acc2=0.887 acc3=0.855 total_acc=0.646\n",
      "134:loss=1.578 acc0=0.952 acc1=0.824 acc2=0.878 acc3=0.866 total_acc=0.603\n",
      "135:loss=1.578 acc0=0.953 acc1=0.824 acc2=0.890 acc3=0.867 total_acc=0.604\n",
      "136:loss=1.571 acc0=0.954 acc1=0.825 acc2=0.901 acc3=0.864 total_acc=0.614\n",
      "137:loss=1.581 acc0=0.950 acc1=0.814 acc2=0.883 acc3=0.869 total_acc=0.590\n",
      "138:loss=1.576 acc0=0.955 acc1=0.825 acc2=0.885 acc3=0.874 total_acc=0.609\n",
      "139:loss=1.563 acc0=0.954 acc1=0.878 acc2=0.891 acc3=0.865 total_acc=0.668\n",
      "140:loss=1.582 acc0=0.949 acc1=0.805 acc2=0.890 acc3=0.872 total_acc=0.595\n",
      "141:loss=1.567 acc0=0.951 acc1=0.882 acc2=0.898 acc3=0.872 total_acc=0.672\n",
      "142:loss=1.592 acc0=0.956 acc1=0.830 acc2=0.819 acc3=0.868 total_acc=0.608\n",
      "143:loss=1.579 acc0=0.944 acc1=0.822 acc2=0.893 acc3=0.865 total_acc=0.605\n",
      "144:loss=1.561 acc0=0.959 acc1=0.887 acc2=0.886 acc3=0.868 total_acc=0.675\n",
      "145:loss=1.577 acc0=0.943 acc1=0.865 acc2=0.871 acc3=0.873 total_acc=0.641\n",
      "146:loss=1.602 acc0=0.944 acc1=0.806 acc2=0.824 acc3=0.866 total_acc=0.594\n",
      "147:loss=1.595 acc0=0.949 acc1=0.833 acc2=0.875 acc3=0.778 total_acc=0.584\n",
      "148:loss=1.570 acc0=0.944 acc1=0.898 acc2=0.839 acc3=0.856 total_acc=0.600\n",
      "149:loss=1.562 acc0=0.946 acc1=0.886 acc2=0.895 acc3=0.869 total_acc=0.679\n",
      "150:loss=1.567 acc0=0.956 acc1=0.829 acc2=0.917 acc3=0.873 total_acc=0.637\n",
      "151:loss=1.551 acc0=0.954 acc1=0.900 acc2=0.914 acc3=0.872 total_acc=0.702\n",
      "152:loss=1.550 acc0=0.959 acc1=0.900 acc2=0.909 acc3=0.873 total_acc=0.706\n",
      "153:loss=1.546 acc0=0.958 acc1=0.910 acc2=0.918 acc3=0.873 total_acc=0.720\n",
      "154:loss=1.546 acc0=0.958 acc1=0.909 acc2=0.918 acc3=0.872 total_acc=0.716\n",
      "155:loss=1.545 acc0=0.958 acc1=0.915 acc2=0.918 acc3=0.872 total_acc=0.714\n",
      "156:loss=1.544 acc0=0.962 acc1=0.910 acc2=0.921 acc3=0.874 total_acc=0.719\n",
      "157:loss=1.544 acc0=0.960 acc1=0.915 acc2=0.915 acc3=0.876 total_acc=0.718\n",
      "158:loss=1.547 acc0=0.961 acc1=0.909 acc2=0.913 acc3=0.872 total_acc=0.709\n",
      "159:loss=1.545 acc0=0.962 acc1=0.913 acc2=0.913 acc3=0.873 total_acc=0.714\n",
      "160:loss=1.545 acc0=0.960 acc1=0.914 acc2=0.913 acc3=0.873 total_acc=0.718\n",
      "161:loss=1.544 acc0=0.965 acc1=0.915 acc2=0.919 acc3=0.872 total_acc=0.722\n",
      "162:loss=1.546 acc0=0.965 acc1=0.907 acc2=0.914 acc3=0.873 total_acc=0.712\n",
      "163:loss=1.547 acc0=0.964 acc1=0.904 acc2=0.913 acc3=0.876 total_acc=0.717\n",
      "164:loss=1.546 acc0=0.962 acc1=0.911 acc2=0.913 acc3=0.873 total_acc=0.718\n",
      "165:loss=1.547 acc0=0.961 acc1=0.905 acc2=0.915 acc3=0.870 total_acc=0.712\n",
      "166:loss=1.546 acc0=0.962 acc1=0.909 acc2=0.917 acc3=0.871 total_acc=0.720\n",
      "167:loss=1.545 acc0=0.961 acc1=0.913 acc2=0.917 acc3=0.873 total_acc=0.722\n",
      "168:loss=1.545 acc0=0.964 acc1=0.905 acc2=0.918 acc3=0.873 total_acc=0.717\n",
      "169:loss=1.544 acc0=0.962 acc1=0.907 acc2=0.921 acc3=0.875 total_acc=0.724\n",
      "170:loss=1.544 acc0=0.962 acc1=0.915 acc2=0.918 acc3=0.870 total_acc=0.717\n",
      "171:loss=1.541 acc0=0.959 acc1=0.911 acc2=0.921 acc3=0.883 total_acc=0.723\n",
      "172:loss=1.541 acc0=0.962 acc1=0.920 acc2=0.920 acc3=0.880 total_acc=0.728\n",
      "173:loss=1.541 acc0=0.960 acc1=0.917 acc2=0.921 acc3=0.882 total_acc=0.725\n",
      "174:loss=1.541 acc0=0.959 acc1=0.920 acc2=0.919 acc3=0.880 total_acc=0.729\n",
      "175:loss=1.539 acc0=0.960 acc1=0.921 acc2=0.931 acc3=0.877 total_acc=0.733\n",
      "176:loss=1.545 acc0=0.959 acc1=0.911 acc2=0.917 acc3=0.874 total_acc=0.716\n",
      "177:loss=1.544 acc0=0.960 acc1=0.914 acc2=0.917 acc3=0.878 total_acc=0.722\n",
      "178:loss=1.544 acc0=0.962 acc1=0.923 acc2=0.915 acc3=0.868 total_acc=0.717\n",
      "179:loss=1.543 acc0=0.957 acc1=0.919 acc2=0.919 acc3=0.875 total_acc=0.719\n",
      "180:loss=1.546 acc0=0.956 acc1=0.909 acc2=0.915 acc3=0.877 total_acc=0.712\n",
      "181:loss=1.564 acc0=0.959 acc1=0.834 acc2=0.924 acc3=0.868 total_acc=0.647\n",
      "182:loss=1.546 acc0=0.964 acc1=0.910 acc2=0.916 acc3=0.874 total_acc=0.718\n",
      "183:loss=1.560 acc0=0.966 acc1=0.841 acc2=0.921 acc3=0.878 total_acc=0.661\n",
      "184:loss=1.545 acc0=0.961 acc1=0.916 acc2=0.917 acc3=0.872 total_acc=0.716\n",
      "185:loss=1.543 acc0=0.965 acc1=0.909 acc2=0.917 acc3=0.880 total_acc=0.722\n",
      "186:loss=1.544 acc0=0.954 acc1=0.923 acc2=0.920 acc3=0.870 total_acc=0.719\n",
      "187:loss=1.554 acc0=0.961 acc1=0.848 acc2=0.924 acc3=0.877 total_acc=0.661\n",
      "188:loss=1.552 acc0=0.956 acc1=0.918 acc2=0.922 acc3=0.870 total_acc=0.723\n",
      "189:loss=1.560 acc0=0.957 acc1=0.849 acc2=0.925 acc3=0.873 total_acc=0.655\n",
      "190:loss=1.562 acc0=0.957 acc1=0.845 acc2=0.920 acc3=0.873 total_acc=0.655\n",
      "191:loss=1.542 acc0=0.957 acc1=0.921 acc2=0.923 acc3=0.875 total_acc=0.731\n",
      "192:loss=1.544 acc0=0.959 acc1=0.914 acc2=0.921 acc3=0.874 total_acc=0.728\n",
      "193:loss=1.558 acc0=0.956 acc1=0.850 acc2=0.925 acc3=0.878 total_acc=0.670\n",
      "194:loss=1.558 acc0=0.952 acc1=0.853 acc2=0.924 acc3=0.881 total_acc=0.666\n",
      "195:loss=1.546 acc0=0.956 acc1=0.921 acc2=0.925 acc3=0.879 total_acc=0.728\n",
      "196:loss=1.552 acc0=0.949 acc1=0.902 acc2=0.910 acc3=0.876 total_acc=0.700\n",
      "197:loss=1.562 acc0=0.959 acc1=0.846 acc2=0.911 acc3=0.878 total_acc=0.648\n",
      "198:loss=1.561 acc0=0.960 acc1=0.848 acc2=0.921 acc3=0.870 total_acc=0.646\n",
      "199:loss=1.546 acc0=0.958 acc1=0.911 acc2=0.917 acc3=0.873 total_acc=0.707\n",
      "200:loss=1.542 acc0=0.961 acc1=0.916 acc2=0.924 acc3=0.876 total_acc=0.721\n",
      "201:loss=1.545 acc0=0.965 acc1=0.911 acc2=0.916 acc3=0.873 total_acc=0.715\n",
      "202:loss=1.542 acc0=0.961 acc1=0.917 acc2=0.924 acc3=0.873 total_acc=0.724\n",
      "203:loss=1.539 acc0=0.964 acc1=0.917 acc2=0.927 acc3=0.879 total_acc=0.735\n",
      "204:loss=1.539 acc0=0.964 acc1=0.922 acc2=0.923 acc3=0.881 total_acc=0.733\n",
      "205:loss=1.538 acc0=0.965 acc1=0.923 acc2=0.921 acc3=0.881 total_acc=0.735\n",
      "206:loss=1.538 acc0=0.964 acc1=0.917 acc2=0.929 acc3=0.882 total_acc=0.742\n",
      "207:loss=1.558 acc0=0.958 acc1=0.848 acc2=0.931 acc3=0.876 total_acc=0.664\n",
      "208:loss=1.548 acc0=0.964 acc1=0.915 acc2=0.925 acc3=0.875 total_acc=0.724\n",
      "209:loss=1.554 acc0=0.965 acc1=0.855 acc2=0.927 acc3=0.877 total_acc=0.668\n",
      "210:loss=1.538 acc0=0.965 acc1=0.927 acc2=0.924 acc3=0.878 total_acc=0.742\n",
      "211:loss=1.541 acc0=0.962 acc1=0.918 acc2=0.921 acc3=0.878 total_acc=0.728\n",
      "212:loss=1.534 acc0=0.958 acc1=0.916 acc2=0.915 acc3=0.920 total_acc=0.764\n",
      "213:loss=1.534 acc0=0.962 acc1=0.911 acc2=0.910 acc3=0.935 total_acc=0.769\n",
      "214:loss=1.528 acc0=0.965 acc1=0.900 acc2=0.918 acc3=0.948 total_acc=0.775\n",
      "215:loss=1.532 acc0=0.953 acc1=0.903 acc2=0.915 acc3=0.945 total_acc=0.769\n",
      "216:loss=1.525 acc0=0.958 acc1=0.919 acc2=0.918 acc3=0.947 total_acc=0.791\n",
      "217:loss=1.541 acc0=0.960 acc1=0.847 acc2=0.921 acc3=0.945 total_acc=0.721\n",
      "218:loss=1.525 acc0=0.961 acc1=0.922 acc2=0.908 acc3=0.955 total_acc=0.789\n",
      "219:loss=1.541 acc0=0.960 acc1=0.854 acc2=0.916 acc3=0.947 total_acc=0.721\n",
      "220:loss=1.541 acc0=0.965 acc1=0.848 acc2=0.916 acc3=0.950 total_acc=0.717\n",
      "221:loss=1.530 acc0=0.966 acc1=0.859 acc2=0.924 acc3=0.957 total_acc=0.744\n",
      "222:loss=1.520 acc0=0.965 acc1=0.921 acc2=0.931 acc3=0.943 total_acc=0.800\n",
      "223:loss=1.522 acc0=0.966 acc1=0.918 acc2=0.920 acc3=0.953 total_acc=0.793\n",
      "224:loss=1.520 acc0=0.960 acc1=0.913 acc2=0.930 acc3=0.960 total_acc=0.799\n",
      "225:loss=1.519 acc0=0.964 acc1=0.927 acc2=0.923 acc3=0.954 total_acc=0.804\n",
      "226:loss=1.536 acc0=0.966 acc1=0.852 acc2=0.919 acc3=0.959 total_acc=0.740\n",
      "227:loss=1.534 acc0=0.971 acc1=0.857 acc2=0.922 acc3=0.959 total_acc=0.745\n",
      "228:loss=1.537 acc0=0.964 acc1=0.858 acc2=0.917 acc3=0.959 total_acc=0.733\n",
      "229:loss=1.534 acc0=0.966 acc1=0.863 acc2=0.920 acc3=0.961 total_acc=0.738\n",
      "230:loss=1.535 acc0=0.966 acc1=0.857 acc2=0.922 acc3=0.959 total_acc=0.741\n",
      "231:loss=1.531 acc0=0.964 acc1=0.863 acc2=0.929 acc3=0.964 total_acc=0.745\n",
      "232:loss=1.534 acc0=0.966 acc1=0.858 acc2=0.924 acc3=0.959 total_acc=0.734\n",
      "233:loss=1.536 acc0=0.969 acc1=0.851 acc2=0.924 acc3=0.956 total_acc=0.742\n",
      "234:loss=1.534 acc0=0.968 acc1=0.857 acc2=0.923 acc3=0.959 total_acc=0.744\n",
      "235:loss=1.535 acc0=0.964 acc1=0.857 acc2=0.922 acc3=0.962 total_acc=0.740\n",
      "236:loss=1.532 acc0=0.967 acc1=0.857 acc2=0.926 acc3=0.969 total_acc=0.752\n",
      "237:loss=1.534 acc0=0.967 acc1=0.853 acc2=0.923 acc3=0.965 total_acc=0.745\n",
      "238:loss=1.536 acc0=0.964 acc1=0.854 acc2=0.918 acc3=0.961 total_acc=0.736\n",
      "239:loss=1.538 acc0=0.962 acc1=0.851 acc2=0.915 acc3=0.959 total_acc=0.727\n",
      "240:loss=1.536 acc0=0.968 acc1=0.859 acc2=0.921 acc3=0.954 total_acc=0.738\n",
      "241:loss=1.545 acc0=0.965 acc1=0.845 acc2=0.904 acc3=0.951 total_acc=0.715\n",
      "242:loss=1.538 acc0=0.966 acc1=0.853 acc2=0.919 acc3=0.953 total_acc=0.726\n",
      "243:loss=1.539 acc0=0.966 acc1=0.846 acc2=0.921 acc3=0.952 total_acc=0.729\n",
      "244:loss=1.538 acc0=0.970 acc1=0.848 acc2=0.920 acc3=0.951 total_acc=0.729\n",
      "245:loss=1.535 acc0=0.964 acc1=0.853 acc2=0.924 acc3=0.961 total_acc=0.742\n",
      "246:loss=1.518 acc0=0.961 acc1=0.925 acc2=0.930 acc3=0.958 total_acc=0.806\n",
      "247:loss=1.520 acc0=0.962 acc1=0.923 acc2=0.926 acc3=0.952 total_acc=0.798\n",
      "248:loss=1.533 acc0=0.966 acc1=0.857 acc2=0.931 acc3=0.960 total_acc=0.749\n",
      "249:loss=1.533 acc0=0.967 acc1=0.864 acc2=0.927 acc3=0.953 total_acc=0.745\n",
      "250:loss=1.533 acc0=0.969 acc1=0.867 acc2=0.920 acc3=0.954 total_acc=0.745\n",
      "251:loss=1.535 acc0=0.965 acc1=0.857 acc2=0.917 acc3=0.965 total_acc=0.743\n",
      "252:loss=1.534 acc0=0.971 acc1=0.858 acc2=0.920 acc3=0.962 total_acc=0.743\n",
      "253:loss=1.533 acc0=0.971 acc1=0.851 acc2=0.926 acc3=0.961 total_acc=0.746\n",
      "254:loss=1.535 acc0=0.970 acc1=0.855 acc2=0.922 acc3=0.957 total_acc=0.732\n",
      "255:loss=1.531 acc0=0.971 acc1=0.864 acc2=0.925 acc3=0.960 total_acc=0.747\n",
      "256:loss=1.532 acc0=0.972 acc1=0.858 acc2=0.930 acc3=0.957 total_acc=0.747\n",
      "257:loss=1.532 acc0=0.970 acc1=0.865 acc2=0.926 acc3=0.955 total_acc=0.747\n",
      "258:loss=1.531 acc0=0.970 acc1=0.865 acc2=0.923 acc3=0.959 total_acc=0.742\n",
      "259:loss=1.533 acc0=0.972 acc1=0.861 acc2=0.921 acc3=0.954 total_acc=0.741\n",
      "260:loss=1.516 acc0=0.970 acc1=0.929 acc2=0.922 acc3=0.966 total_acc=0.822\n",
      "261:loss=1.530 acc0=0.970 acc1=0.864 acc2=0.919 acc3=0.965 total_acc=0.744\n",
      "262:loss=1.523 acc0=0.971 acc1=0.868 acc2=0.930 acc3=0.966 total_acc=0.761\n",
      "263:loss=1.514 acc0=0.969 acc1=0.933 acc2=0.925 acc3=0.968 total_acc=0.826\n",
      "264:loss=1.515 acc0=0.962 acc1=0.935 acc2=0.926 acc3=0.961 total_acc=0.821\n",
      "265:loss=1.514 acc0=0.965 acc1=0.934 acc2=0.923 acc3=0.966 total_acc=0.816\n",
      "266:loss=1.515 acc0=0.968 acc1=0.941 acc2=0.916 acc3=0.962 total_acc=0.816\n",
      "267:loss=1.515 acc0=0.967 acc1=0.929 acc2=0.922 acc3=0.964 total_acc=0.814\n",
      "268:loss=1.516 acc0=0.966 acc1=0.932 acc2=0.926 acc3=0.957 total_acc=0.814\n",
      "269:loss=1.516 acc0=0.968 acc1=0.922 acc2=0.932 acc3=0.959 total_acc=0.816\n",
      "270:loss=1.516 acc0=0.970 acc1=0.924 acc2=0.926 acc3=0.960 total_acc=0.814\n",
      "271:loss=1.521 acc0=0.962 acc1=0.925 acc2=0.923 acc3=0.951 total_acc=0.799\n",
      "272:loss=1.516 acc0=0.969 acc1=0.923 acc2=0.932 acc3=0.957 total_acc=0.818\n",
      "273:loss=1.517 acc0=0.967 acc1=0.921 acc2=0.920 acc3=0.965 total_acc=0.807\n",
      "274:loss=1.519 acc0=0.960 acc1=0.927 acc2=0.914 acc3=0.966 total_acc=0.797\n",
      "275:loss=1.518 acc0=0.965 acc1=0.924 acc2=0.922 acc3=0.964 total_acc=0.807\n",
      "276:loss=1.518 acc0=0.964 acc1=0.926 acc2=0.917 acc3=0.966 total_acc=0.808\n",
      "277:loss=1.517 acc0=0.964 acc1=0.925 acc2=0.928 acc3=0.957 total_acc=0.809\n",
      "278:loss=1.519 acc0=0.961 acc1=0.922 acc2=0.924 acc3=0.961 total_acc=0.807\n",
      "279:loss=1.547 acc0=0.951 acc1=0.824 acc2=0.918 acc3=0.965 total_acc=0.707\n",
      "280:loss=1.539 acc0=0.957 acc1=0.844 acc2=0.925 acc3=0.959 total_acc=0.727\n",
      "281:loss=1.515 acc0=0.967 acc1=0.926 acc2=0.927 acc3=0.966 total_acc=0.814\n",
      "282:loss=1.520 acc0=0.964 acc1=0.919 acc2=0.919 acc3=0.959 total_acc=0.799\n",
      "283:loss=1.541 acc0=0.964 acc1=0.850 acc2=0.913 acc3=0.949 total_acc=0.718\n",
      "284:loss=1.519 acc0=0.962 acc1=0.925 acc2=0.921 acc3=0.961 total_acc=0.806\n",
      "285:loss=1.518 acc0=0.964 acc1=0.928 acc2=0.913 acc3=0.966 total_acc=0.803\n",
      "286:loss=1.521 acc0=0.967 acc1=0.922 acc2=0.918 acc3=0.957 total_acc=0.800\n",
      "287:loss=1.520 acc0=0.965 acc1=0.924 acc2=0.921 acc3=0.955 total_acc=0.802\n",
      "288:loss=1.518 acc0=0.965 acc1=0.925 acc2=0.922 acc3=0.960 total_acc=0.805\n",
      "289:loss=1.516 acc0=0.969 acc1=0.928 acc2=0.928 acc3=0.955 total_acc=0.811\n",
      "290:loss=1.517 acc0=0.971 acc1=0.929 acc2=0.923 acc3=0.956 total_acc=0.811\n",
      "291:loss=1.536 acc0=0.965 acc1=0.855 acc2=0.924 acc3=0.956 total_acc=0.741\n",
      "292:loss=1.521 acc0=0.968 acc1=0.921 acc2=0.916 acc3=0.958 total_acc=0.801\n",
      "293:loss=1.519 acc0=0.968 acc1=0.924 acc2=0.921 acc3=0.957 total_acc=0.803\n",
      "294:loss=1.517 acc0=0.965 acc1=0.924 acc2=0.935 acc3=0.954 total_acc=0.810\n",
      "295:loss=1.531 acc0=0.964 acc1=0.861 acc2=0.931 acc3=0.962 total_acc=0.749\n",
      "296:loss=1.531 acc0=0.971 acc1=0.865 acc2=0.933 acc3=0.954 total_acc=0.750\n",
      "297:loss=1.531 acc0=0.969 acc1=0.870 acc2=0.928 acc3=0.956 total_acc=0.750\n",
      "298:loss=1.534 acc0=0.967 acc1=0.860 acc2=0.923 acc3=0.956 total_acc=0.742\n",
      "299:loss=1.536 acc0=0.966 acc1=0.867 acc2=0.917 acc3=0.951 total_acc=0.743\n",
      "300:loss=1.531 acc0=0.971 acc1=0.861 acc2=0.925 acc3=0.960 total_acc=0.749\n",
      "301:loss=1.532 acc0=0.969 acc1=0.859 acc2=0.923 acc3=0.962 total_acc=0.746\n",
      "302:loss=1.533 acc0=0.969 acc1=0.855 acc2=0.926 acc3=0.960 total_acc=0.742\n",
      "303:loss=1.533 acc0=0.972 acc1=0.856 acc2=0.922 acc3=0.961 total_acc=0.743\n",
      "304:loss=1.532 acc0=0.972 acc1=0.855 acc2=0.924 acc3=0.960 total_acc=0.740\n",
      "305:loss=1.532 acc0=0.972 acc1=0.858 acc2=0.923 acc3=0.961 total_acc=0.742\n",
      "306:loss=1.532 acc0=0.971 acc1=0.860 acc2=0.923 acc3=0.964 total_acc=0.745\n",
      "307:loss=1.533 acc0=0.967 acc1=0.857 acc2=0.924 acc3=0.962 total_acc=0.743\n",
      "308:loss=1.533 acc0=0.968 acc1=0.859 acc2=0.928 acc3=0.957 total_acc=0.744\n",
      "309:loss=1.530 acc0=0.970 acc1=0.864 acc2=0.930 acc3=0.961 total_acc=0.754\n",
      "310:loss=1.529 acc0=0.969 acc1=0.864 acc2=0.931 acc3=0.965 total_acc=0.757\n",
      "311:loss=1.513 acc0=0.968 acc1=0.930 acc2=0.932 acc3=0.961 total_acc=0.820\n",
      "312:loss=1.513 acc0=0.968 acc1=0.928 acc2=0.936 acc3=0.960 total_acc=0.820\n",
      "313:loss=1.512 acc0=0.970 acc1=0.928 acc2=0.938 acc3=0.962 total_acc=0.827\n",
      "314:loss=1.521 acc0=0.967 acc1=0.863 acc2=0.934 acc3=0.964 total_acc=0.757\n",
      "315:loss=1.517 acc0=0.968 acc1=0.927 acc2=0.934 acc3=0.964 total_acc=0.823\n",
      "316:loss=1.512 acc0=0.968 acc1=0.932 acc2=0.934 acc3=0.961 total_acc=0.828\n",
      "317:loss=1.512 acc0=0.968 acc1=0.931 acc2=0.934 acc3=0.962 total_acc=0.826\n",
      "318:loss=1.512 acc0=0.970 acc1=0.930 acc2=0.932 acc3=0.965 total_acc=0.827\n",
      "319:loss=1.512 acc0=0.970 acc1=0.928 acc2=0.931 acc3=0.965 total_acc=0.826\n",
      "320:loss=1.512 acc0=0.969 acc1=0.930 acc2=0.934 acc3=0.964 total_acc=0.829\n",
      "321:loss=1.511 acc0=0.971 acc1=0.932 acc2=0.935 acc3=0.961 total_acc=0.830\n",
      "322:loss=1.511 acc0=0.971 acc1=0.931 acc2=0.934 acc3=0.962 total_acc=0.830\n",
      "323:loss=1.512 acc0=0.971 acc1=0.929 acc2=0.932 acc3=0.964 total_acc=0.828\n",
      "324:loss=1.513 acc0=0.969 acc1=0.927 acc2=0.929 acc3=0.965 total_acc=0.823\n",
      "325:loss=1.513 acc0=0.969 acc1=0.931 acc2=0.928 acc3=0.964 total_acc=0.824\n",
      "326:loss=1.514 acc0=0.969 acc1=0.930 acc2=0.928 acc3=0.960 total_acc=0.823\n",
      "327:loss=1.514 acc0=0.968 acc1=0.930 acc2=0.929 acc3=0.961 total_acc=0.824\n",
      "328:loss=1.513 acc0=0.967 acc1=0.932 acc2=0.931 acc3=0.965 total_acc=0.831\n",
      "329:loss=1.513 acc0=0.967 acc1=0.933 acc2=0.930 acc3=0.964 total_acc=0.829\n",
      "330:loss=1.511 acc0=0.967 acc1=0.935 acc2=0.931 acc3=0.966 total_acc=0.834\n",
      "331:loss=1.513 acc0=0.969 acc1=0.930 acc2=0.930 acc3=0.960 total_acc=0.823\n",
      "332:loss=1.514 acc0=0.970 acc1=0.931 acc2=0.927 acc3=0.962 total_acc=0.825\n",
      "333:loss=1.514 acc0=0.970 acc1=0.929 acc2=0.932 acc3=0.958 total_acc=0.820\n",
      "334:loss=1.515 acc0=0.968 acc1=0.928 acc2=0.929 acc3=0.958 total_acc=0.816\n",
      "335:loss=1.514 acc0=0.970 acc1=0.932 acc2=0.924 acc3=0.964 total_acc=0.818\n",
      "336:loss=1.514 acc0=0.971 acc1=0.932 acc2=0.925 acc3=0.964 total_acc=0.821\n",
      "337:loss=1.514 acc0=0.971 acc1=0.933 acc2=0.924 acc3=0.958 total_acc=0.818\n",
      "338:loss=1.513 acc0=0.971 acc1=0.935 acc2=0.929 acc3=0.959 total_acc=0.826\n",
      "339:loss=1.512 acc0=0.971 acc1=0.938 acc2=0.927 acc3=0.960 total_acc=0.826\n",
      "340:loss=1.512 acc0=0.970 acc1=0.938 acc2=0.928 acc3=0.959 total_acc=0.828\n",
      "341:loss=1.514 acc0=0.968 acc1=0.933 acc2=0.926 acc3=0.961 total_acc=0.821\n",
      "342:loss=1.515 acc0=0.966 acc1=0.929 acc2=0.928 acc3=0.960 total_acc=0.821\n",
      "343:loss=1.514 acc0=0.968 acc1=0.929 acc2=0.931 acc3=0.962 total_acc=0.826\n",
      "344:loss=1.514 acc0=0.968 acc1=0.930 acc2=0.932 acc3=0.962 total_acc=0.825\n",
      "345:loss=1.513 acc0=0.968 acc1=0.928 acc2=0.933 acc3=0.961 total_acc=0.823\n",
      "346:loss=1.513 acc0=0.969 acc1=0.930 acc2=0.931 acc3=0.961 total_acc=0.825\n",
      "347:loss=1.513 acc0=0.968 acc1=0.928 acc2=0.931 acc3=0.961 total_acc=0.823\n",
      "348:loss=1.513 acc0=0.968 acc1=0.931 acc2=0.932 acc3=0.965 total_acc=0.828\n",
      "349:loss=1.512 acc0=0.968 acc1=0.930 acc2=0.936 acc3=0.962 total_acc=0.829\n",
      "350:loss=1.513 acc0=0.966 acc1=0.933 acc2=0.928 acc3=0.965 total_acc=0.824\n",
      "351:loss=1.513 acc0=0.966 acc1=0.933 acc2=0.926 acc3=0.964 total_acc=0.822\n",
      "352:loss=1.514 acc0=0.968 acc1=0.932 acc2=0.930 acc3=0.961 total_acc=0.826\n",
      "353:loss=1.514 acc0=0.966 acc1=0.930 acc2=0.928 acc3=0.961 total_acc=0.817\n",
      "354:loss=1.515 acc0=0.971 acc1=0.925 acc2=0.924 acc3=0.960 total_acc=0.821\n",
      "355:loss=1.513 acc0=0.967 acc1=0.933 acc2=0.928 acc3=0.960 total_acc=0.828\n",
      "356:loss=1.513 acc0=0.967 acc1=0.929 acc2=0.933 acc3=0.961 total_acc=0.828\n",
      "357:loss=1.512 acc0=0.970 acc1=0.930 acc2=0.932 acc3=0.961 total_acc=0.831\n",
      "358:loss=1.513 acc0=0.971 acc1=0.925 acc2=0.931 acc3=0.960 total_acc=0.825\n",
      "359:loss=1.512 acc0=0.971 acc1=0.930 acc2=0.933 acc3=0.961 total_acc=0.831\n",
      "360:loss=1.512 acc0=0.969 acc1=0.932 acc2=0.932 acc3=0.962 total_acc=0.829\n",
      "361:loss=1.512 acc0=0.970 acc1=0.931 acc2=0.930 acc3=0.960 total_acc=0.829\n",
      "362:loss=1.511 acc0=0.967 acc1=0.932 acc2=0.936 acc3=0.959 total_acc=0.831\n",
      "363:loss=1.511 acc0=0.967 acc1=0.931 acc2=0.936 acc3=0.959 total_acc=0.830\n",
      "364:loss=1.512 acc0=0.970 acc1=0.931 acc2=0.932 acc3=0.960 total_acc=0.831\n",
      "365:loss=1.512 acc0=0.970 acc1=0.933 acc2=0.933 acc3=0.959 total_acc=0.831\n",
      "366:loss=1.511 acc0=0.967 acc1=0.932 acc2=0.938 acc3=0.962 total_acc=0.836\n",
      "367:loss=1.511 acc0=0.967 acc1=0.932 acc2=0.938 acc3=0.962 total_acc=0.836\n",
      "368:loss=1.511 acc0=0.969 acc1=0.930 acc2=0.935 acc3=0.964 total_acc=0.834\n",
      "369:loss=1.511 acc0=0.970 acc1=0.929 acc2=0.935 acc3=0.961 total_acc=0.831\n",
      "370:loss=1.512 acc0=0.971 acc1=0.929 acc2=0.934 acc3=0.964 total_acc=0.834\n",
      "371:loss=1.510 acc0=0.970 acc1=0.934 acc2=0.939 acc3=0.962 total_acc=0.840\n",
      "372:loss=1.510 acc0=0.968 acc1=0.933 acc2=0.942 acc3=0.961 total_acc=0.840\n",
      "373:loss=1.511 acc0=0.966 acc1=0.934 acc2=0.939 acc3=0.960 total_acc=0.834\n",
      "374:loss=1.511 acc0=0.968 acc1=0.933 acc2=0.934 acc3=0.962 total_acc=0.832\n",
      "375:loss=1.511 acc0=0.967 acc1=0.935 acc2=0.932 acc3=0.965 total_acc=0.829\n",
      "376:loss=1.511 acc0=0.969 acc1=0.935 acc2=0.929 acc3=0.966 total_acc=0.832\n",
      "377:loss=1.512 acc0=0.966 acc1=0.938 acc2=0.931 acc3=0.964 total_acc=0.829\n",
      "378:loss=1.514 acc0=0.964 acc1=0.931 acc2=0.931 acc3=0.964 total_acc=0.825\n",
      "379:loss=1.513 acc0=0.965 acc1=0.933 acc2=0.930 acc3=0.964 total_acc=0.829\n",
      "380:loss=1.513 acc0=0.964 acc1=0.930 acc2=0.935 acc3=0.962 total_acc=0.828\n",
      "381:loss=1.512 acc0=0.970 acc1=0.930 acc2=0.936 acc3=0.961 total_acc=0.831\n",
      "382:loss=1.512 acc0=0.969 acc1=0.927 acc2=0.936 acc3=0.967 total_acc=0.835\n",
      "383:loss=1.511 acc0=0.967 acc1=0.930 acc2=0.938 acc3=0.965 total_acc=0.834\n",
      "384:loss=1.510 acc0=0.971 acc1=0.930 acc2=0.939 acc3=0.965 total_acc=0.837\n",
      "385:loss=1.508 acc0=0.974 acc1=0.932 acc2=0.936 acc3=0.969 total_acc=0.844\n",
      "386:loss=1.510 acc0=0.974 acc1=0.930 acc2=0.935 acc3=0.964 total_acc=0.837\n",
      "387:loss=1.511 acc0=0.974 acc1=0.930 acc2=0.935 acc3=0.964 total_acc=0.837\n",
      "388:loss=1.510 acc0=0.973 acc1=0.933 acc2=0.935 acc3=0.964 total_acc=0.840\n",
      "389:loss=1.512 acc0=0.969 acc1=0.931 acc2=0.930 acc3=0.967 total_acc=0.834\n",
      "390:loss=1.531 acc0=0.967 acc1=0.865 acc2=0.925 acc3=0.965 total_acc=0.760\n",
      "391:loss=1.515 acc0=0.968 acc1=0.933 acc2=0.932 acc3=0.966 total_acc=0.839\n",
      "392:loss=1.522 acc0=0.971 acc1=0.864 acc2=0.930 acc3=0.967 total_acc=0.770\n",
      "393:loss=1.509 acc0=0.973 acc1=0.933 acc2=0.932 acc3=0.969 total_acc=0.842\n",
      "394:loss=1.512 acc0=0.965 acc1=0.933 acc2=0.933 acc3=0.966 total_acc=0.829\n",
      "395:loss=1.510 acc0=0.968 acc1=0.934 acc2=0.933 acc3=0.968 total_acc=0.836\n",
      "396:loss=1.511 acc0=0.969 acc1=0.933 acc2=0.930 acc3=0.964 total_acc=0.828\n",
      "397:loss=1.511 acc0=0.967 acc1=0.935 acc2=0.932 acc3=0.965 total_acc=0.831\n",
      "398:loss=1.515 acc0=0.967 acc1=0.925 acc2=0.930 acc3=0.964 total_acc=0.822\n",
      "399:loss=1.514 acc0=0.965 acc1=0.926 acc2=0.930 acc3=0.964 total_acc=0.820\n",
      "400:loss=1.511 acc0=0.970 acc1=0.933 acc2=0.926 acc3=0.970 total_acc=0.830\n",
      "401:loss=1.512 acc0=0.969 acc1=0.932 acc2=0.927 acc3=0.965 total_acc=0.826\n",
      "402:loss=1.513 acc0=0.968 acc1=0.932 acc2=0.927 acc3=0.965 total_acc=0.824\n",
      "403:loss=1.513 acc0=0.970 acc1=0.931 acc2=0.927 acc3=0.965 total_acc=0.826\n",
      "404:loss=1.513 acc0=0.970 acc1=0.931 acc2=0.927 acc3=0.965 total_acc=0.826\n",
      "405:loss=1.514 acc0=0.967 acc1=0.930 acc2=0.931 acc3=0.966 total_acc=0.830\n",
      "406:loss=1.514 acc0=0.968 acc1=0.929 acc2=0.929 acc3=0.965 total_acc=0.827\n",
      "407:loss=1.531 acc0=0.969 acc1=0.861 acc2=0.928 acc3=0.964 total_acc=0.758\n",
      "408:loss=1.531 acc0=0.968 acc1=0.865 acc2=0.925 acc3=0.967 total_acc=0.760\n",
      "409:loss=1.531 acc0=0.967 acc1=0.861 acc2=0.929 acc3=0.965 total_acc=0.760\n",
      "410:loss=1.530 acc0=0.968 acc1=0.865 acc2=0.926 acc3=0.967 total_acc=0.760\n",
      "411:loss=1.515 acc0=0.968 acc1=0.931 acc2=0.926 acc3=0.968 total_acc=0.828\n",
      "412:loss=1.511 acc0=0.968 acc1=0.938 acc2=0.929 acc3=0.967 total_acc=0.831\n",
      "413:loss=1.511 acc0=0.969 acc1=0.935 acc2=0.932 acc3=0.967 total_acc=0.833\n",
      "414:loss=1.512 acc0=0.966 acc1=0.934 acc2=0.929 acc3=0.967 total_acc=0.829\n",
      "415:loss=1.512 acc0=0.966 acc1=0.935 acc2=0.925 acc3=0.968 total_acc=0.824\n",
      "416:loss=1.511 acc0=0.966 acc1=0.936 acc2=0.931 acc3=0.968 total_acc=0.832\n",
      "417:loss=1.512 acc0=0.968 acc1=0.938 acc2=0.927 acc3=0.966 total_acc=0.823\n",
      "418:loss=1.512 acc0=0.969 acc1=0.936 acc2=0.931 acc3=0.962 total_acc=0.824\n",
      "419:loss=1.513 acc0=0.965 acc1=0.931 acc2=0.928 acc3=0.966 total_acc=0.824\n",
      "420:loss=1.529 acc0=0.964 acc1=0.865 acc2=0.930 acc3=0.968 total_acc=0.759\n",
      "421:loss=1.514 acc0=0.966 acc1=0.933 acc2=0.922 acc3=0.968 total_acc=0.818\n",
      "422:loss=1.514 acc0=0.970 acc1=0.932 acc2=0.920 acc3=0.967 total_acc=0.820\n",
      "423:loss=1.515 acc0=0.965 acc1=0.931 acc2=0.922 acc3=0.965 total_acc=0.814\n",
      "424:loss=1.514 acc0=0.967 acc1=0.929 acc2=0.931 acc3=0.967 total_acc=0.826\n",
      "425:loss=1.510 acc0=0.969 acc1=0.933 acc2=0.927 acc3=0.970 total_acc=0.830\n",
      "426:loss=1.511 acc0=0.969 acc1=0.932 acc2=0.926 acc3=0.970 total_acc=0.828\n",
      "427:loss=1.510 acc0=0.967 acc1=0.939 acc2=0.930 acc3=0.970 total_acc=0.833\n",
      "428:loss=1.510 acc0=0.967 acc1=0.938 acc2=0.930 acc3=0.969 total_acc=0.831\n",
      "429:loss=1.510 acc0=0.967 acc1=0.938 acc2=0.931 acc3=0.969 total_acc=0.832\n",
      "430:loss=1.511 acc0=0.965 acc1=0.938 acc2=0.930 acc3=0.969 total_acc=0.830\n",
      "431:loss=1.511 acc0=0.965 acc1=0.934 acc2=0.930 acc3=0.968 total_acc=0.828\n",
      "432:loss=1.511 acc0=0.966 acc1=0.932 acc2=0.934 acc3=0.969 total_acc=0.830\n",
      "433:loss=1.512 acc0=0.966 acc1=0.930 acc2=0.928 acc3=0.967 total_acc=0.820\n",
      "434:loss=1.512 acc0=0.967 acc1=0.932 acc2=0.928 acc3=0.970 total_acc=0.825\n",
      "435:loss=1.511 acc0=0.967 acc1=0.936 acc2=0.932 acc3=0.968 total_acc=0.830\n",
      "436:loss=1.511 acc0=0.970 acc1=0.932 acc2=0.933 acc3=0.965 total_acc=0.827\n",
      "437:loss=1.508 acc0=0.970 acc1=0.939 acc2=0.935 acc3=0.970 total_acc=0.843\n",
      "438:loss=1.508 acc0=0.970 acc1=0.938 acc2=0.935 acc3=0.969 total_acc=0.842\n",
      "439:loss=1.508 acc0=0.970 acc1=0.936 acc2=0.934 acc3=0.971 total_acc=0.842\n",
      "440:loss=1.508 acc0=0.971 acc1=0.935 acc2=0.935 acc3=0.970 total_acc=0.843\n",
      "441:loss=1.508 acc0=0.972 acc1=0.941 acc2=0.936 acc3=0.967 total_acc=0.847\n",
      "442:loss=1.507 acc0=0.971 acc1=0.939 acc2=0.934 acc3=0.971 total_acc=0.845\n",
      "443:loss=1.508 acc0=0.972 acc1=0.932 acc2=0.936 acc3=0.975 total_acc=0.844\n",
      "444:loss=1.510 acc0=0.971 acc1=0.930 acc2=0.930 acc3=0.968 total_acc=0.832\n",
      "445:loss=1.511 acc0=0.970 acc1=0.933 acc2=0.927 acc3=0.968 total_acc=0.826\n",
      "446:loss=1.511 acc0=0.969 acc1=0.932 acc2=0.928 acc3=0.968 total_acc=0.824\n",
      "447:loss=1.511 acc0=0.974 acc1=0.928 acc2=0.931 acc3=0.966 total_acc=0.827\n",
      "448:loss=1.511 acc0=0.974 acc1=0.928 acc2=0.931 acc3=0.965 total_acc=0.826\n",
      "449:loss=1.512 acc0=0.972 acc1=0.928 acc2=0.931 acc3=0.967 total_acc=0.828\n",
      "450:loss=1.511 acc0=0.972 acc1=0.928 acc2=0.931 acc3=0.967 total_acc=0.828\n",
      "451:loss=1.511 acc0=0.972 acc1=0.928 acc2=0.930 acc3=0.967 total_acc=0.827\n",
      "452:loss=1.511 acc0=0.972 acc1=0.928 acc2=0.931 acc3=0.967 total_acc=0.828\n",
      "453:loss=1.511 acc0=0.972 acc1=0.928 acc2=0.931 acc3=0.966 total_acc=0.827\n",
      "454:loss=1.511 acc0=0.973 acc1=0.926 acc2=0.933 acc3=0.967 total_acc=0.828\n",
      "455:loss=1.511 acc0=0.973 acc1=0.927 acc2=0.934 acc3=0.968 total_acc=0.831\n",
      "456:loss=1.510 acc0=0.973 acc1=0.927 acc2=0.934 acc3=0.967 total_acc=0.830\n",
      "457:loss=1.511 acc0=0.972 acc1=0.928 acc2=0.931 acc3=0.965 total_acc=0.825\n",
      "458:loss=1.512 acc0=0.972 acc1=0.928 acc2=0.930 acc3=0.965 total_acc=0.824\n",
      "459:loss=1.511 acc0=0.974 acc1=0.927 acc2=0.932 acc3=0.967 total_acc=0.829\n",
      "460:loss=1.511 acc0=0.973 acc1=0.928 acc2=0.929 acc3=0.967 total_acc=0.826\n",
      "461:loss=1.511 acc0=0.973 acc1=0.927 acc2=0.932 acc3=0.967 total_acc=0.828\n",
      "462:loss=1.511 acc0=0.973 acc1=0.926 acc2=0.932 acc3=0.967 total_acc=0.827\n",
      "463:loss=1.509 acc0=0.974 acc1=0.930 acc2=0.934 acc3=0.970 total_acc=0.835\n",
      "464:loss=1.509 acc0=0.974 acc1=0.930 acc2=0.934 acc3=0.970 total_acc=0.835\n",
      "465:loss=1.509 acc0=0.974 acc1=0.930 acc2=0.936 acc3=0.970 total_acc=0.839\n",
      "466:loss=1.508 acc0=0.974 acc1=0.930 acc2=0.936 acc3=0.971 total_acc=0.839\n",
      "467:loss=1.509 acc0=0.973 acc1=0.932 acc2=0.931 acc3=0.970 total_acc=0.832\n",
      "468:loss=1.509 acc0=0.973 acc1=0.932 acc2=0.931 acc3=0.971 total_acc=0.834\n",
      "469:loss=1.509 acc0=0.973 acc1=0.932 acc2=0.932 acc3=0.971 total_acc=0.835\n",
      "470:loss=1.509 acc0=0.973 acc1=0.932 acc2=0.932 acc3=0.971 total_acc=0.835\n",
      "471:loss=1.509 acc0=0.972 acc1=0.932 acc2=0.931 acc3=0.970 total_acc=0.834\n",
      "472:loss=1.509 acc0=0.973 acc1=0.933 acc2=0.930 acc3=0.969 total_acc=0.834\n",
      "473:loss=1.509 acc0=0.973 acc1=0.933 acc2=0.929 acc3=0.970 total_acc=0.833\n",
      "474:loss=1.510 acc0=0.972 acc1=0.931 acc2=0.930 acc3=0.970 total_acc=0.830\n",
      "475:loss=1.510 acc0=0.972 acc1=0.931 acc2=0.930 acc3=0.970 total_acc=0.830\n",
      "476:loss=1.509 acc0=0.973 acc1=0.932 acc2=0.930 acc3=0.971 total_acc=0.833\n",
      "477:loss=1.509 acc0=0.972 acc1=0.932 acc2=0.931 acc3=0.971 total_acc=0.833\n",
      "478:loss=1.509 acc0=0.972 acc1=0.932 acc2=0.931 acc3=0.971 total_acc=0.832\n",
      "479:loss=1.509 acc0=0.970 acc1=0.935 acc2=0.932 acc3=0.971 total_acc=0.836\n",
      "480:loss=1.509 acc0=0.970 acc1=0.935 acc2=0.931 acc3=0.971 total_acc=0.835\n",
      "481:loss=1.510 acc0=0.970 acc1=0.932 acc2=0.934 acc3=0.968 total_acc=0.835\n",
      "482:loss=1.510 acc0=0.972 acc1=0.932 acc2=0.934 acc3=0.968 total_acc=0.835\n",
      "483:loss=1.510 acc0=0.972 acc1=0.931 acc2=0.935 acc3=0.968 total_acc=0.835\n",
      "484:loss=1.510 acc0=0.972 acc1=0.931 acc2=0.935 acc3=0.968 total_acc=0.835\n",
      "485:loss=1.509 acc0=0.973 acc1=0.931 acc2=0.935 acc3=0.969 total_acc=0.835\n",
      "486:loss=1.509 acc0=0.972 acc1=0.933 acc2=0.935 acc3=0.969 total_acc=0.836\n",
      "487:loss=1.509 acc0=0.973 acc1=0.931 acc2=0.935 acc3=0.970 total_acc=0.835\n",
      "488:loss=1.509 acc0=0.973 acc1=0.931 acc2=0.936 acc3=0.969 total_acc=0.835\n",
      "489:loss=1.509 acc0=0.973 acc1=0.931 acc2=0.936 acc3=0.969 total_acc=0.835\n",
      "490:loss=1.509 acc0=0.972 acc1=0.931 acc2=0.936 acc3=0.968 total_acc=0.834\n",
      "491:loss=1.509 acc0=0.970 acc1=0.931 acc2=0.935 acc3=0.970 total_acc=0.834\n",
      "492:loss=1.509 acc0=0.971 acc1=0.930 acc2=0.935 acc3=0.969 total_acc=0.833\n",
      "493:loss=1.509 acc0=0.971 acc1=0.931 acc2=0.936 acc3=0.970 total_acc=0.835\n",
      "494:loss=1.510 acc0=0.971 acc1=0.932 acc2=0.935 acc3=0.969 total_acc=0.834\n",
      "495:loss=1.510 acc0=0.971 acc1=0.932 acc2=0.935 acc3=0.969 total_acc=0.834\n",
      "496:loss=1.510 acc0=0.971 acc1=0.932 acc2=0.933 acc3=0.966 total_acc=0.830\n",
      "497:loss=1.511 acc0=0.970 acc1=0.932 acc2=0.933 acc3=0.966 total_acc=0.830\n",
      "498:loss=1.509 acc0=0.971 acc1=0.932 acc2=0.935 acc3=0.969 total_acc=0.835\n",
      "499:loss=1.509 acc0=0.971 acc1=0.932 acc2=0.938 acc3=0.968 total_acc=0.835\n"
     ]
    }
   ],
   "source": [
    "# 调试自己的代码\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "# 错误代码，防止将生成代码再重新执行一遍\n",
    "print(ss)\n",
    "\n",
    "# 定义基本参数\n",
    "dataset_dir = './images/'            # 验证码存放文件夹\n",
    "num_test = 0.2                       # 测试集占20%\n",
    "batch_size = 64    # 01不同\n",
    "epochs = 500                         # 训练100个周期\n",
    "num_classes = 10                     # 分类数为10\n",
    "lr = tf.Variable(0.001, dtype=tf.float32)    # 学习率\n",
    "is_training = tf.placeholder(tf.bool)        # 是否为训练状态，tf.bool: True/False\n",
    "\n",
    "\n",
    "# 获取所有验证码图片路径和标签\n",
    "def get_images_paths_and_labels(dataset_dir):\n",
    "    labels = []\n",
    "    image_path = []\n",
    "    for filenames in os.listdir(dataset_dir):\n",
    "        path = os.path.join(dataset_dir, filenames)\n",
    "        image_path.append(path)\n",
    "        image_label = filenames.split('.')[0]    # 02不同   # 如['1234']\n",
    "        image_labels = []                        # 如[1, 2, 3, 4]\n",
    "        for i in range(len(image_label)):\n",
    "            image_labels.append(int(image_label[i]))\n",
    "        labels.append(image_labels)\n",
    "    return image_path, labels\n",
    "\n",
    "\n",
    "image_path, labels = get_images_paths_and_labels(dataset_dir)\n",
    "# 将得到的列表转换为数组，方便后面的打乱数据\n",
    "image_path = np.array(image_path)\n",
    "labels = np.array(labels)\n",
    "# 打乱数据\n",
    "np.random.seed(2019)    # 03不同\n",
    "shuffle_indices = np.random.permutation(np.arange(len(image_path)))    # np.random.permutation()打乱数组，但不在原数组上进行\n",
    "image_path_shuffled = image_path[shuffle_indices]                      # 按打乱的索引重新取数据，即打乱数据\n",
    "labels_shuffled = labels[shuffle_indices]\n",
    "# 切分训练集和测试集\n",
    "test_sample_index = -1*int(num_test*float(len(image_path)))\n",
    "x_train, x_test = image_path_shuffled[:test_sample_index], image_path_shuffled[test_sample_index:]\n",
    "y_train, y_test = labels_shuffled[:test_sample_index], labels_shuffled[test_sample_index:]\n",
    "\n",
    "\n",
    "# 图像处理函数\n",
    "def parse_function(filenames, images_labels=None):\n",
    "    image = tf.read_file(filenames)                    # 读取文件\n",
    "    image = tf.image.decode_jpeg(image, channels=3)    # 对图像文件解码\n",
    "    image = tf.image.resize_images(image, [224, 224])  # 将图片重新定义大小，因为后面用的是AlexNet网络，图片大小为224*224\n",
    "    # 图片预处理\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "    return image, images_labels\n",
    "\n",
    "\n",
    "features_placeholder = tf.placeholder(image_path_shuffled.dtype, [None])\n",
    "labels_placeholder = tf.placeholder(labels_shuffled.dtype, [None, 4])\n",
    "# 创建dataset对象, 类似于管道机制，需要配合Iterator进行使用\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "dataset = dataset.map(parse_function)    # 处理图片\n",
    "dataset = dataset.repeat(1)              # 训练周期为1，即将所有数据遍历一次\n",
    "dataset = dataset.batch(batch_size)      # 批次大小\n",
    "iterator = dataset.make_initializable_iterator()    # 初始化迭代器\n",
    "data_batch, label_batch = iterator.get_next()\n",
    "\n",
    "\n",
    "def AlexNet(inputs, is_training=True):\n",
    "    '''定义AlexNet神经网络结构，该网络获得2012年的图像识别冠军'''\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,\n",
    "                        weights_initializer=tf.glorot_uniform_initializer(),\n",
    "                        biases_initializer=tf.constant_initializer(0)):    # 会自动初始化所有的权重\n",
    "\n",
    "#         AlexNet网络，但是精确度不高\n",
    "#         net = slim.conv2d(inputs, 96, [11, 11], 4)\n",
    "#         net = slim.max_pool2d(net, [3, 3])\n",
    "#         net = slim.conv2d(net, 256, [5, 5])\n",
    "#         net = slim.max_pool2d(net, [3, 3])\n",
    "#         net = slim.conv2d(net, 384, [5, 5])\n",
    "#         net = slim.conv2d(net, 384, [3, 3])\n",
    "#         net = slim.conv2d(net, 256, [3, 3])\n",
    "#         net = slim.max_pool2d(net, [3, 3])\n",
    "\n",
    "        # 自定义的神经网络结构\n",
    "        net = slim.conv2d(inputs, 64, [11, 11], 4)    # 4: stride\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        net = slim.conv2d(net, 192, [5, 5])\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.conv2d(net, 384, [3, 3])\n",
    "        net = slim.conv2d(net, 256, [3, 3])    # 04不同 256\n",
    "        net = slim.max_pool2d(net, [3, 3])\n",
    "\n",
    "        # 数据扁平化\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 4096)    # 05不同 1024\n",
    "        net = slim.dropout(net, is_training=is_training)\n",
    "\n",
    "        net0 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net1 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net2 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "        net3 = slim.fully_connected(net, num_classes, activation_fn=tf.nn.softmax)\n",
    "\n",
    "    return net0, net1, net2, net3\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 传入数据得到结果\n",
    "    logit0, logit1, logit2, logit3 = AlexNet(data_batch, is_training)\n",
    "    # 定义loss\n",
    "    ## sparse_softmax_cross_etropy: 标签为整数；\n",
    "    ## softmax_cross_entropy: 标签为one-hot编码\n",
    "    loss0 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 0], logit0)\n",
    "    loss1 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 1], logit1)\n",
    "    loss2 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 2], logit2)\n",
    "    loss3 = tf.losses.sparse_softmax_cross_entropy(label_batch[:, 3], logit3)\n",
    "    total_loss = (loss0 + loss1 + loss2 + loss3) / 4.0\n",
    "\n",
    "    # 优化total_loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(total_loss)\n",
    "\n",
    "    correct0 = tf.nn.in_top_k(logit0, label_batch[:, 0], 1)\n",
    "    accuracy0 = tf.reduce_mean(tf.cast(correct0, tf.float32))\n",
    "    correct1 = tf.nn.in_top_k(logit1, label_batch[:, 1], 1)\n",
    "    accuracy1 = tf.reduce_mean(tf.cast(correct1, tf.float32))\n",
    "    correct2 = tf.nn.in_top_k(logit2, label_batch[:, 2], 1)\n",
    "    accuracy2 = tf.reduce_mean(tf.cast(correct2, tf.float32))\n",
    "    correct3 = tf.nn.in_top_k(logit3, label_batch[:, 3], 1)\n",
    "    accuracy3 = tf.reduce_mean(tf.cast(correct3, tf.float32))\n",
    "    total_correct = tf.cast(correct0, tf.float32)*tf.cast(correct1, tf.float32)*tf.cast(correct2, tf.float32)*tf.cast(correct3, tf.float32)\n",
    "    total_accuracy = tf.reduce_mean(tf.cast(total_correct, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    for i in range(epochs):\n",
    "        if i%150==0:\n",
    "            sess.run(tf.assign(lr, lr/3))\n",
    "\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: x_train, labels_placeholder: y_train})\n",
    "        l = 1\n",
    "        while True:\n",
    "            try:\n",
    "                sess.run(optimizer, feed_dict={is_training: True})\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                # 训练完所有的数据后则跳出循环\n",
    "                break\n",
    "\n",
    "        sess.run(iterator.initializer, feed_dict={features_placeholder: x_test, labels_placeholder: y_test})\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                acc0, acc1, acc2, acc3, total_acc, los = sess.run([accuracy0, accuracy1, accuracy2, accuracy3,\n",
    "                                                                   total_accuracy, total_loss],\n",
    "                                                                  feed_dict={is_training: False})\n",
    "                tf.add_to_collection('sum_losses', los)    # tf.add_to_collection()把多个变量放入一个自己y用引号命名的集合里，也就是把多个变量统一放在一个列表中\n",
    "                tf.add_to_collection('accuracy0', acc0)\n",
    "                tf.add_to_collection('accuracy1', acc1)\n",
    "                tf.add_to_collection('accuracy2', acc2)\n",
    "                tf.add_to_collection('accuracy3', acc3)\n",
    "                tf.add_to_collection('total_accuracy', total_acc)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                avg_loss = sess.run(tf.reduce_mean(tf.get_collection('sum_losses')))    # tf.get_collection()从一个结合中取出全部变量，是一个列表\n",
    "                avg_acc0 = sess.run(tf.reduce_mean(tf.get_collection('accuracy0')))\n",
    "                avg_acc1 = sess.run(tf.reduce_mean(tf.get_collection('accuracy1')))\n",
    "                avg_acc2 = sess.run(tf.reduce_mean(tf.get_collection('accuracy2')))\n",
    "                avg_acc3 = sess.run(tf.reduce_mean(tf.get_collection('accuracy3')))\n",
    "                avg_total_acc = sess.run(tf.reduce_mean(tf.get_collection('total_accuracy')))\n",
    "                print('%d:loss=%.3f acc0=%.3f acc1=%.3f acc2=%.3f acc3=%.3f total_acc=%.3f' %(i,avg_loss,avg_acc0,avg_acc1,avg_acc2,avg_acc3,avg_total_acc))\n",
    "                temp = tf.get_collection_ref('sum_losses')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy0')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy1')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy2')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('accuracy3')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                temp = tf.get_collection_ref('total_accuracy')    # 清空loss统计\n",
    "                del temp[:]\n",
    "                break\n",
    "    saver.save(sess, 'models/model.ckpt')    # 07 不同\n",
    "#         saver.save(sess, 'models/model.ckpt', global_step=10)    # global_step: 在n次迭代后再保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "heading_collapsed": true,
    "id": "tPLSRtpW5saV"
   },
   "source": [
    "###### 总结\n",
    "没有使用AlexNet模型，使用的网络结构相比于AlexNet稍微有些变动，在第250个周期时，这个模型的准确率是0.745，最终完成500个周期时，模型的准确率为0.835，表明训练周期越长，准确率会相对提升一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 总结\n",
    "通过训练不同的周期发现，训练周期越长，准确率相对来说就越高；  \n",
    "在训练的过程中，发现了学习率衰减的问题，具体的可以参考吴恩达深度学习课程中所提到的学习率衰减的办法。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03_4位数字验证码识别(完整测试版).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "445px",
    "left": "1092.81px",
    "top": "146px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "476px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
