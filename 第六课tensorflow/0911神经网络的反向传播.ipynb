{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码看着长，但是其实并不是很长，分为三大块主要内容：  \n",
    "- 前向传播的过程  \n",
    "- 反向传播更新参数的过程  \n",
    "- 预测  \n",
    "\n",
    "备注：面试题一般只会考单隐层的神经网络结构，而单层的神经网络结构有无法对复杂的非线性函数进行很好的拟合（主要是因为单隐层的神经网络没办法非线性，主要是取决于激活函数），所以要考的话也只能是单层的线性函数的拟合，这个是可以解决的（不在后面添加激活函数即可）；重点还是**反向传播求导更新参数**的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 定义sigmoide函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88079708, 0.88079708, 0.88079708, 0.73105858, 0.73105858,\n",
       "       0.73105858, 0.88079708, 0.88079708, 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.random.randint(1,3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 初始化参数\n",
    "将参数初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "    dim：向量的维度，或特征的个数。\n",
    "    返回值\n",
    "    w：初始化的向量，维度(dim, 1)\n",
    "    b：初始化的截距。\n",
    "    \"\"\"\n",
    "    w = np.zeros(shape=(dim,1))\n",
    "    b = 0\n",
    "    assert w.shape == (dim,1)\n",
    "    assert isinstance(b, int) or isinstance(b,float)\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 前向传播\n",
    "前序传播和反向传播函数是用来学习参数的函数。其过程如下：\n",
    "\n",
    "- 传入$X$\n",
    "- 计算 $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- 计算损失函数 $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "- 计算参数的相对梯度（一阶导数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 传播函数\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "    w：数组，权重。\n",
    "    b：浮点型，偏差值。\n",
    "    X：特征数据\n",
    "    Y：数据标签。\n",
    "    Return:\n",
    "    cost：损失值\n",
    "    dw：关于参数w的梯度，维度与w相同。\n",
    "    db：关于参数b的梯度，维度与b相同。\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    # 前向传播\n",
    "    A = sigmoid(np.dot(w.T,X)+b)    \n",
    "    cost = -1./m * np.sum(np.multiply(np.log(A), Y) + np.multiply(np.log(1-A),1-Y))# compute cost\n",
    "    \n",
    "    dw = None\n",
    "    db = None\n",
    "    # 反向传播\n",
    "    dZ = A-Y\n",
    "    dw = 1./m * np.dot(dZ, X.T)\n",
    "    dw = dw.T\n",
    "    db = 1./m * np.sum(dZ, keepdims=True, axis=1)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = [[0.00145558]]\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 优化参数\n",
    "\n",
    "目标是通过最小化损失函数$J$来学习$w$和$b$。 对参数$\\theta$，更新规则$ \\theta = \\theta - \\alpha \\text{ } d\\theta$, 其中$\\alpha$是学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化函数\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "    w：数组，权重。\n",
    "    b：浮点型，偏差值。\n",
    "    X：数组，特征数据\n",
    "    Y：数组， 数据标签。\n",
    "    num_iterations：整型，迭代次数\n",
    "    learning_rate：浮点型，学习率\n",
    "    print_cost：布尔型，是否打印损失值。\n",
    "    Returns:\n",
    "    params：字典，参数w和b\n",
    "    grads：字典，参数w和b的梯度\n",
    "    costs：列表，记录迭代过程中的损失值。\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    for i in range(num_iterations):\n",
    "        # 损失和梯度计算\n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        # 相对梯度\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        # 更新参数\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # 记录损失值\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # 打印损失值\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 预测\n",
    "通过公式$\\hat{Y} = A = \\sigma(w^T X + b)$进行预测，设定0.5为阈值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    Arguments:\n",
    "    w：数组，权重\n",
    "    b：标量，截距\n",
    "    X：数组，带预测数据\n",
    "    Returns:\n",
    "    Y_prediction：数组，预测结果（非概率结果，是分类结果。大于或等于0.5为1，小于0.5为0）\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # 计算预测值，并得出分类\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    for i in range(X.shape[0]):\n",
    "        Y_prediction[0,i] = 0 if A[0][i]<0.5 else 1\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    print(A)\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52241976 0.50960677 0.34597965]]\n",
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
